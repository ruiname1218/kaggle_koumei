{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seed_cell"
      },
      "outputs": [],
      "source": [
        "SEED = 5000\n",
        "import os, random, numpy as np\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "# PyTorch seeding (TensorFlow is skipped to avoid environment issues)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(SEED)\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:34.996422Z",
          "iopub.status.busy": "2024-10-31T16:06:34.995779Z",
          "iopub.status.idle": "2024-10-31T16:06:35.006468Z",
          "shell.execute_reply": "2024-10-31T16:06:35.005154Z",
          "shell.execute_reply.started": "2024-10-31T16:06:34.996355Z"
        },
        "id": "w0miHB_4j6fO",
        "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\n",
            "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\n",
            "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.009259Z",
          "iopub.status.busy": "2024-10-31T16:06:35.008896Z",
          "iopub.status.idle": "2024-10-31T16:06:35.040481Z",
          "shell.execute_reply": "2024-10-31T16:06:35.039428Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.009221Z"
        },
        "id": "wmcUA-Kyj6fR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
        "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
        "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.042311Z",
          "iopub.status.busy": "2024-10-31T16:06:35.041938Z",
          "iopub.status.idle": "2024-10-31T16:06:35.084905Z",
          "shell.execute_reply": "2024-10-31T16:06:35.083678Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.042267Z"
        },
        "id": "4lpnr8L4j6fR",
        "outputId": "9954614d-7fa8-44ac-c804-a226ef527424",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Lat_Dec</th>\n",
              "      <th>Lon_Dec</th>\n",
              "      <th>NO2uM</th>\n",
              "      <th>NO3uM</th>\n",
              "      <th>NH3uM</th>\n",
              "      <th>R_TEMP</th>\n",
              "      <th>R_Depth</th>\n",
              "      <th>R_Sal</th>\n",
              "      <th>R_DYNHT</th>\n",
              "      <th>R_Nuts</th>\n",
              "      <th>R_Oxy_micromol.Kg</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "      <th>PO4uM</th>\n",
              "      <th>SiO3uM</th>\n",
              "      <th>TA1.x</th>\n",
              "      <th>Salinity1</th>\n",
              "      <th>Temperature_degC</th>\n",
              "      <th>DIC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>34.385030</td>\n",
              "      <td>-120.665530</td>\n",
              "      <td>0.030</td>\n",
              "      <td>33.80</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.79</td>\n",
              "      <td>323</td>\n",
              "      <td>141.2</td>\n",
              "      <td>0.642</td>\n",
              "      <td>0.00</td>\n",
              "      <td>37.40948</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.77</td>\n",
              "      <td>53.86</td>\n",
              "      <td>2287.45</td>\n",
              "      <td>34.1980</td>\n",
              "      <td>7.82</td>\n",
              "      <td>2270.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>31.418333</td>\n",
              "      <td>-121.998333</td>\n",
              "      <td>0.000</td>\n",
              "      <td>34.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.12</td>\n",
              "      <td>323</td>\n",
              "      <td>140.8</td>\n",
              "      <td>0.767</td>\n",
              "      <td>0.00</td>\n",
              "      <td>64.81441</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.57</td>\n",
              "      <td>52.50</td>\n",
              "      <td>2279.10</td>\n",
              "      <td>34.0740</td>\n",
              "      <td>7.15</td>\n",
              "      <td>2254.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>34.385030</td>\n",
              "      <td>-120.665530</td>\n",
              "      <td>0.180</td>\n",
              "      <td>14.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.68</td>\n",
              "      <td>50</td>\n",
              "      <td>246.8</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.00</td>\n",
              "      <td>180.29150</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.29</td>\n",
              "      <td>13.01</td>\n",
              "      <td>2230.80</td>\n",
              "      <td>33.5370</td>\n",
              "      <td>11.68</td>\n",
              "      <td>2111.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>33.482580</td>\n",
              "      <td>-122.533070</td>\n",
              "      <td>0.013</td>\n",
              "      <td>29.67</td>\n",
              "      <td>0.01</td>\n",
              "      <td>8.33</td>\n",
              "      <td>232</td>\n",
              "      <td>158.5</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.01</td>\n",
              "      <td>89.62595</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.27</td>\n",
              "      <td>38.98</td>\n",
              "      <td>2265.85</td>\n",
              "      <td>34.0480</td>\n",
              "      <td>8.36</td>\n",
              "      <td>2223.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>31.414320</td>\n",
              "      <td>-121.997670</td>\n",
              "      <td>0.000</td>\n",
              "      <td>33.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>7.53</td>\n",
              "      <td>323</td>\n",
              "      <td>143.4</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.05</td>\n",
              "      <td>60.03062</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.53</td>\n",
              "      <td>49.28</td>\n",
              "      <td>2278.49</td>\n",
              "      <td>34.1170</td>\n",
              "      <td>7.57</td>\n",
              "      <td>2252.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1449</th>\n",
              "      <td>1450</td>\n",
              "      <td>33.420000</td>\n",
              "      <td>-117.901667</td>\n",
              "      <td>0.000</td>\n",
              "      <td>38.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.52</td>\n",
              "      <td>520</td>\n",
              "      <td>118.2</td>\n",
              "      <td>0.889</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12.17673</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.11</td>\n",
              "      <td>73.10</td>\n",
              "      <td>2313.52</td>\n",
              "      <td>34.3024</td>\n",
              "      <td>6.57</td>\n",
              "      <td>2311.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>1451</td>\n",
              "      <td>34.271667</td>\n",
              "      <td>-120.023333</td>\n",
              "      <td>0.330</td>\n",
              "      <td>5.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>13.21</td>\n",
              "      <td>49</td>\n",
              "      <td>290.1</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.00</td>\n",
              "      <td>224.81120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.75</td>\n",
              "      <td>6.50</td>\n",
              "      <td>2223.31</td>\n",
              "      <td>33.3304</td>\n",
              "      <td>13.22</td>\n",
              "      <td>2048.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1451</th>\n",
              "      <td>1452</td>\n",
              "      <td>31.410000</td>\n",
              "      <td>-121.991667</td>\n",
              "      <td>0.000</td>\n",
              "      <td>33.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.61</td>\n",
              "      <td>322</td>\n",
              "      <td>143.1</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.00</td>\n",
              "      <td>60.46569</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.56</td>\n",
              "      <td>49.50</td>\n",
              "      <td>2280.43</td>\n",
              "      <td>34.1269</td>\n",
              "      <td>7.64</td>\n",
              "      <td>2251.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>1453</td>\n",
              "      <td>34.274270</td>\n",
              "      <td>-120.030570</td>\n",
              "      <td>0.517</td>\n",
              "      <td>1.58</td>\n",
              "      <td>0.35</td>\n",
              "      <td>14.47</td>\n",
              "      <td>30</td>\n",
              "      <td>319.0</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.35</td>\n",
              "      <td>249.28420</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.54</td>\n",
              "      <td>4.12</td>\n",
              "      <td>2223.88</td>\n",
              "      <td>33.2660</td>\n",
              "      <td>14.47</td>\n",
              "      <td>2030.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1453</th>\n",
              "      <td>1454</td>\n",
              "      <td>32.421030</td>\n",
              "      <td>-119.963580</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>16.84</td>\n",
              "      <td>3</td>\n",
              "      <td>352.0</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.05</td>\n",
              "      <td>247.19050</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.67</td>\n",
              "      <td>2237.74</td>\n",
              "      <td>33.4960</td>\n",
              "      <td>16.84</td>\n",
              "      <td>2015.34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1454 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id    Lat_Dec     Lon_Dec  NO2uM  NO3uM  NH3uM  R_TEMP  R_Depth  \\\n",
              "0        1  34.385030 -120.665530  0.030  33.80   0.00    7.79      323   \n",
              "1        2  31.418333 -121.998333  0.000  34.70   0.00    7.12      323   \n",
              "2        3  34.385030 -120.665530  0.180  14.20   0.00   11.68       50   \n",
              "3        4  33.482580 -122.533070  0.013  29.67   0.01    8.33      232   \n",
              "4        5  31.414320 -121.997670  0.000  33.10   0.05    7.53      323   \n",
              "...    ...        ...         ...    ...    ...    ...     ...      ...   \n",
              "1449  1450  33.420000 -117.901667  0.000  38.60   0.00    6.52      520   \n",
              "1450  1451  34.271667 -120.023333  0.330   5.90   0.00   13.21       49   \n",
              "1451  1452  31.410000 -121.991667  0.000  33.60   0.00    7.61      322   \n",
              "1452  1453  34.274270 -120.030570  0.517   1.58   0.35   14.47       30   \n",
              "1453  1454  32.421030 -119.963580  0.006   0.00   0.05   16.84        3   \n",
              "\n",
              "      R_Sal  R_DYNHT  R_Nuts  R_Oxy_micromol.Kg  Unnamed: 12  PO4uM  SiO3uM  \\\n",
              "0     141.2    0.642    0.00           37.40948          NaN   2.77   53.86   \n",
              "1     140.8    0.767    0.00           64.81441          NaN   2.57   52.50   \n",
              "2     246.8    0.144    0.00          180.29150          NaN   1.29   13.01   \n",
              "3     158.5    0.562    0.01           89.62595          NaN   2.27   38.98   \n",
              "4     143.4    0.740    0.05           60.03062          NaN   2.53   49.28   \n",
              "...     ...      ...     ...                ...          ...    ...     ...   \n",
              "1449  118.2    0.889    0.00           12.17673          NaN   3.11   73.10   \n",
              "1450  290.1    0.149    0.00          224.81120          NaN   0.75    6.50   \n",
              "1451  143.1    0.731    0.00           60.46569          NaN   2.56   49.50   \n",
              "1452  319.0    0.110    0.35          249.28420          NaN   0.54    4.12   \n",
              "1453  352.0    0.011    0.05          247.19050          NaN   0.32    0.67   \n",
              "\n",
              "        TA1.x  Salinity1  Temperature_degC      DIC  \n",
              "0     2287.45    34.1980              7.82  2270.17  \n",
              "1     2279.10    34.0740              7.15  2254.10  \n",
              "2     2230.80    33.5370             11.68  2111.04  \n",
              "3     2265.85    34.0480              8.36  2223.41  \n",
              "4     2278.49    34.1170              7.57  2252.62  \n",
              "...       ...        ...               ...      ...  \n",
              "1449  2313.52    34.3024              6.57  2311.19  \n",
              "1450  2223.31    33.3304             13.22  2048.93  \n",
              "1451  2280.43    34.1269              7.64  2251.34  \n",
              "1452  2223.88    33.2660             14.47  2030.03  \n",
              "1453  2237.74    33.4960             16.84  2015.34  \n",
              "\n",
              "[1454 rows x 19 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.087790Z",
          "iopub.status.busy": "2024-10-31T16:06:35.087373Z",
          "iopub.status.idle": "2024-10-31T16:06:35.115106Z",
          "shell.execute_reply": "2024-10-31T16:06:35.113668Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.087751Z"
        },
        "id": "fvZCm9pEj6fR",
        "outputId": "6d3137dd-7d85-421b-fd3c-3cd3cb4b9859",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Lat_Dec</th>\n",
              "      <th>Lon_Dec</th>\n",
              "      <th>NO2uM</th>\n",
              "      <th>NO3uM</th>\n",
              "      <th>NH3uM</th>\n",
              "      <th>R_TEMP</th>\n",
              "      <th>R_Depth</th>\n",
              "      <th>R_Sal</th>\n",
              "      <th>R_DYNHT</th>\n",
              "      <th>R_Nuts</th>\n",
              "      <th>R_Oxy_micromol.Kg</th>\n",
              "      <th>PO4uM</th>\n",
              "      <th>SiO3uM</th>\n",
              "      <th>TA1</th>\n",
              "      <th>Salinity1</th>\n",
              "      <th>Temperature_degC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1455</td>\n",
              "      <td>34.321666</td>\n",
              "      <td>-120.811666</td>\n",
              "      <td>0.020</td>\n",
              "      <td>24.00</td>\n",
              "      <td>0.41</td>\n",
              "      <td>9.51</td>\n",
              "      <td>101</td>\n",
              "      <td>189.9</td>\n",
              "      <td>0.258</td>\n",
              "      <td>0.41</td>\n",
              "      <td>138.838300</td>\n",
              "      <td>1.85</td>\n",
              "      <td>25.50</td>\n",
              "      <td>2244.94</td>\n",
              "      <td>33.830</td>\n",
              "      <td>9.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1456</td>\n",
              "      <td>34.275000</td>\n",
              "      <td>-120.033333</td>\n",
              "      <td>0.000</td>\n",
              "      <td>25.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.84</td>\n",
              "      <td>102</td>\n",
              "      <td>185.2</td>\n",
              "      <td>0.264</td>\n",
              "      <td>0.00</td>\n",
              "      <td>102.709200</td>\n",
              "      <td>2.06</td>\n",
              "      <td>28.30</td>\n",
              "      <td>2253.27</td>\n",
              "      <td>33.963</td>\n",
              "      <td>9.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1457</td>\n",
              "      <td>34.275000</td>\n",
              "      <td>-120.033333</td>\n",
              "      <td>0.000</td>\n",
              "      <td>31.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.60</td>\n",
              "      <td>514</td>\n",
              "      <td>124.1</td>\n",
              "      <td>0.874</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.174548</td>\n",
              "      <td>3.40</td>\n",
              "      <td>88.10</td>\n",
              "      <td>2316.95</td>\n",
              "      <td>34.241</td>\n",
              "      <td>6.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1458</td>\n",
              "      <td>33.828333</td>\n",
              "      <td>-118.625000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.20</td>\n",
              "      <td>19.21</td>\n",
              "      <td>1</td>\n",
              "      <td>408.1</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.20</td>\n",
              "      <td>258.674300</td>\n",
              "      <td>0.27</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2240.49</td>\n",
              "      <td>33.465</td>\n",
              "      <td>19.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1459</td>\n",
              "      <td>33.828333</td>\n",
              "      <td>-118.625000</td>\n",
              "      <td>0.020</td>\n",
              "      <td>19.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.65</td>\n",
              "      <td>100</td>\n",
              "      <td>215.5</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.00</td>\n",
              "      <td>145.839900</td>\n",
              "      <td>1.64</td>\n",
              "      <td>19.40</td>\n",
              "      <td>2238.30</td>\n",
              "      <td>33.720</td>\n",
              "      <td>10.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>1935</td>\n",
              "      <td>31.418030</td>\n",
              "      <td>-121.989970</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>21.20</td>\n",
              "      <td>2</td>\n",
              "      <td>465.3</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.01</td>\n",
              "      <td>229.490000</td>\n",
              "      <td>0.28</td>\n",
              "      <td>1.89</td>\n",
              "      <td>2235.34</td>\n",
              "      <td>33.380</td>\n",
              "      <td>21.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>1936</td>\n",
              "      <td>31.418030</td>\n",
              "      <td>-121.989970</td>\n",
              "      <td>0.000</td>\n",
              "      <td>24.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.54</td>\n",
              "      <td>232</td>\n",
              "      <td>171.3</td>\n",
              "      <td>0.689</td>\n",
              "      <td>0.00</td>\n",
              "      <td>153.330000</td>\n",
              "      <td>1.81</td>\n",
              "      <td>29.40</td>\n",
              "      <td>2250.00</td>\n",
              "      <td>33.934</td>\n",
              "      <td>8.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>1937</td>\n",
              "      <td>31.418030</td>\n",
              "      <td>-121.989970</td>\n",
              "      <td>0.000</td>\n",
              "      <td>33.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.15</td>\n",
              "      <td>323</td>\n",
              "      <td>144.0</td>\n",
              "      <td>0.829</td>\n",
              "      <td>0.00</td>\n",
              "      <td>78.890000</td>\n",
              "      <td>2.51</td>\n",
              "      <td>50.75</td>\n",
              "      <td>2270.19</td>\n",
              "      <td>34.049</td>\n",
              "      <td>7.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>1938</td>\n",
              "      <td>32.846330</td>\n",
              "      <td>-117.531300</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.05</td>\n",
              "      <td>13.95</td>\n",
              "      <td>30</td>\n",
              "      <td>307.5</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.05</td>\n",
              "      <td>259.410000</td>\n",
              "      <td>0.49</td>\n",
              "      <td>3.52</td>\n",
              "      <td>2225.36</td>\n",
              "      <td>33.285</td>\n",
              "      <td>13.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>1939</td>\n",
              "      <td>32.846330</td>\n",
              "      <td>-117.531300</td>\n",
              "      <td>0.025</td>\n",
              "      <td>27.45</td>\n",
              "      <td>0.02</td>\n",
              "      <td>10.01</td>\n",
              "      <td>232</td>\n",
              "      <td>168.9</td>\n",
              "      <td>0.537</td>\n",
              "      <td>0.02</td>\n",
              "      <td>54.310000</td>\n",
              "      <td>2.43</td>\n",
              "      <td>34.90</td>\n",
              "      <td>2279.15</td>\n",
              "      <td>34.287</td>\n",
              "      <td>10.03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>485 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id    Lat_Dec     Lon_Dec  NO2uM  NO3uM  NH3uM  R_TEMP  R_Depth  R_Sal  \\\n",
              "0    1455  34.321666 -120.811666  0.020  24.00   0.41    9.51      101  189.9   \n",
              "1    1456  34.275000 -120.033333  0.000  25.10   0.00    9.84      102  185.2   \n",
              "2    1457  34.275000 -120.033333  0.000  31.90   0.00    6.60      514  124.1   \n",
              "3    1458  33.828333 -118.625000  0.000   0.00   0.20   19.21        1  408.1   \n",
              "4    1459  33.828333 -118.625000  0.020  19.70   0.00   10.65      100  215.5   \n",
              "..    ...        ...         ...    ...    ...    ...     ...      ...    ...   \n",
              "480  1935  31.418030 -121.989970  0.000   0.00   0.01   21.20        2  465.3   \n",
              "481  1936  31.418030 -121.989970  0.000  24.50   0.00    8.54      232  171.3   \n",
              "482  1937  31.418030 -121.989970  0.000  33.70   0.00    7.15      323  144.0   \n",
              "483  1938  32.846330 -117.531300  0.037   0.75   0.05   13.95       30  307.5   \n",
              "484  1939  32.846330 -117.531300  0.025  27.45   0.02   10.01      232  168.9   \n",
              "\n",
              "     R_DYNHT  R_Nuts  R_Oxy_micromol.Kg  PO4uM  SiO3uM      TA1  Salinity1  \\\n",
              "0      0.258    0.41         138.838300   1.85   25.50  2244.94     33.830   \n",
              "1      0.264    0.00         102.709200   2.06   28.30  2253.27     33.963   \n",
              "2      0.874    0.00           2.174548   3.40   88.10  2316.95     34.241   \n",
              "3      0.004    0.20         258.674300   0.27    2.50  2240.49     33.465   \n",
              "4      0.274    0.00         145.839900   1.64   19.40  2238.30     33.720   \n",
              "..       ...     ...                ...    ...     ...      ...        ...   \n",
              "480    0.009    0.01         229.490000   0.28    1.89  2235.34     33.380   \n",
              "481    0.689    0.00         153.330000   1.81   29.40  2250.00     33.934   \n",
              "482    0.829    0.00          78.890000   2.51   50.75  2270.19     34.049   \n",
              "483    0.117    0.05         259.410000   0.49    3.52  2225.36     33.285   \n",
              "484    0.537    0.02          54.310000   2.43   34.90  2279.15     34.287   \n",
              "\n",
              "     Temperature_degC  \n",
              "0                9.52  \n",
              "1                9.85  \n",
              "2                6.65  \n",
              "3               19.21  \n",
              "4               10.66  \n",
              "..                ...  \n",
              "480             21.20  \n",
              "481              8.57  \n",
              "482              7.18  \n",
              "483             13.95  \n",
              "484             10.03  \n",
              "\n",
              "[485 rows x 17 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.117373Z",
          "iopub.status.busy": "2024-10-31T16:06:35.116945Z",
          "iopub.status.idle": "2024-10-31T16:06:35.143455Z",
          "shell.execute_reply": "2024-10-31T16:06:35.142299Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.117323Z"
        },
        "id": "lwgCh6Taj6fS",
        "outputId": "01d8cfa0-97f2-4c82-e9ab-05bb91b988b1",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1454 entries, 0 to 1453\n",
            "Data columns (total 19 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 1454 non-null   int64  \n",
            " 1   Lat_Dec            1454 non-null   float64\n",
            " 2   Lon_Dec            1454 non-null   float64\n",
            " 3   NO2uM              1454 non-null   float64\n",
            " 4   NO3uM              1454 non-null   float64\n",
            " 5   NH3uM              1454 non-null   float64\n",
            " 6   R_TEMP             1454 non-null   float64\n",
            " 7   R_Depth            1454 non-null   int64  \n",
            " 8   R_Sal              1454 non-null   float64\n",
            " 9   R_DYNHT            1454 non-null   float64\n",
            " 10  R_Nuts             1454 non-null   float64\n",
            " 11  R_Oxy_micromol.Kg  1454 non-null   float64\n",
            " 12  Unnamed: 12        0 non-null      float64\n",
            " 13  PO4uM              1454 non-null   float64\n",
            " 14  SiO3uM             1454 non-null   float64\n",
            " 15  TA1.x              1454 non-null   float64\n",
            " 16  Salinity1          1454 non-null   float64\n",
            " 17  Temperature_degC   1454 non-null   float64\n",
            " 18  DIC                1454 non-null   float64\n",
            "dtypes: float64(17), int64(2)\n",
            "memory usage: 216.0 KB\n"
          ]
        }
      ],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.145181Z",
          "iopub.status.busy": "2024-10-31T16:06:35.144822Z",
          "iopub.status.idle": "2024-10-31T16:06:35.157578Z",
          "shell.execute_reply": "2024-10-31T16:06:35.156065Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.145148Z"
        },
        "id": "3wIs9caWj6fS",
        "outputId": "4ad10a3f-4ab2-4f4a-bada-e3253191f450",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>DIC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1455</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1456</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1457</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1458</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1459</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>1935</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>1936</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>1937</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>1938</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>1939</td>\n",
              "      <td>2150.46882</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>485 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id         DIC\n",
              "0    1455  2150.46882\n",
              "1    1456  2150.46882\n",
              "2    1457  2150.46882\n",
              "3    1458  2150.46882\n",
              "4    1459  2150.46882\n",
              "..    ...         ...\n",
              "480  1935  2150.46882\n",
              "481  1936  2150.46882\n",
              "482  1937  2150.46882\n",
              "483  1938  2150.46882\n",
              "484  1939  2150.46882\n",
              "\n",
              "[485 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.160063Z",
          "iopub.status.busy": "2024-10-31T16:06:35.159559Z",
          "iopub.status.idle": "2024-10-31T16:06:38.637873Z",
          "shell.execute_reply": "2024-10-31T16:06:38.636808Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.160004Z"
        },
        "id": "2SYojsJUj6fS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler, QuantileTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "# train = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
        "# test = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
        "\n",
        "# Drop unneeded columns and handle missing values\n",
        "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
        "\n",
        "# Find common columns between train and test (excluding \"DIC\" from train)\n",
        "common_columns = train.drop(columns=[\"DIC\"]).columns.intersection(test.columns)\n",
        "\n",
        "# Select the common columns for both train and test\n",
        "X = train[common_columns].copy()\n",
        "y = train[\"DIC\"]\n",
        "test = test[common_columns].copy()\n",
        "\n",
        "\n",
        "# Toggle for feature engineering (set to False to disable)\n",
        "# === 調整ポイント: 特徴量ENGのON/OFF（効果比較に便利） ===\n",
        "# True: 追加特徴（N_tot, 比率, 交互作用, sin/cos 等）を使う\n",
        "# False: 生の共通列のみで学習\n",
        "FE_ENABLED = False\n",
        "\n",
        "if FE_ENABLED:\n",
        "    # Feature engineering (deterministic; same for train/test)\n",
        "    # Totals and ratios (avoid divide-by-zero -> NaN; imputer will handle)\n",
        "    X['N_tot'] = X['NO3uM'] + X['NO2uM'] + X['NH3uM']\n",
        "    test['N_tot'] = test['NO3uM'] + test['NO2uM'] + test['NH3uM']\n",
        "    X['N_to_P'] = np.where(X['PO4uM'] == 0, np.nan, X['NO3uM'] / X['PO4uM'])\n",
        "    test['N_to_P'] = np.where(test['PO4uM'] == 0, np.nan, test['NO3uM'] / test['PO4uM'])\n",
        "    X['Si_to_N'] = np.where(X['NO3uM'] == 0, np.nan, X['SiO3uM'] / X['NO3uM'])\n",
        "    test['Si_to_N'] = np.where(test['NO3uM'] == 0, np.nan, test['SiO3uM'] / test['NO3uM'])\n",
        "    # Interactions\n",
        "    X['Depth_Temp'] = X['R_Depth'] * X['R_TEMP']\n",
        "    test['Depth_Temp'] = test['R_Depth'] * test['R_TEMP']\n",
        "    X['Sal_Temp'] = X['R_Sal'] * X['R_TEMP']\n",
        "    test['Sal_Temp'] = test['R_Sal'] * test['R_TEMP']\n",
        "    # Geographic trig features\n",
        "    X['sin_lat'] = np.sin(np.radians(X['Lat_Dec']))\n",
        "    X['cos_lat'] = np.cos(np.radians(X['Lat_Dec']))\n",
        "    X['sin_lon'] = np.sin(np.radians(X['Lon_Dec']))\n",
        "    X['cos_lon'] = np.cos(np.radians(X['Lon_Dec']))\n",
        "    test['sin_lat'] = np.sin(np.radians(test['Lat_Dec']))\n",
        "    test['cos_lat'] = np.cos(np.radians(test['Lat_Dec']))\n",
        "    test['sin_lon'] = np.sin(np.radians(test['Lon_Dec']))\n",
        "    test['cos_lon'] = np.cos(np.radians(test['Lon_Dec']))\n",
        "    # Replace infs with NaN to be imputed\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "    test = test.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# ===== 調整ポイント: 前処理/変換スイッチ =====\n",
        "# FEATURE_IMPUTER: 欠損補完の方法 ('mean' または 'median')\n",
        "# FEATURE_TRANSFORM: 特徴量変換\n",
        "#   - 'power'    : Yeo-Johnson + 標準化（0/負値OK, 初手におすすめ）\n",
        "#   - 'standard' : 標準化のみ（平均0・分散1）\n",
        "#   - 'robust'   : 外れ値に頑健なスケーリング\n",
        "#   - 'quantile' : 分位変換（出力分布は下のQUANTILE_OUTPUTで指定）\n",
        "#   - 'none'     : 変換しない\n",
        "# TARGET_TRANSFORM: 目的変数変換 ('none'|'log1p'|'standard')\n",
        "#   - 'log1p'    : スケール/外れ値を抑制。予測は自動でexpm1逆変換\n",
        "#   - 'standard' : 平均0・分散1に正規化（予測は平均・分散で戻す）\n",
        "# 変更後は前処理→学習→推論の順に実行してください\n",
        "# Switches for feature/target transforms\n",
        "FEATURE_IMPUTER = 'mean'  # 'mean' or 'median'\n",
        "FEATURE_TRANSFORM = 'power'  # 'power'|'standard'|'robust'|'quantile'|'none'\n",
        "QUANTILE_OUTPUT = 'normal'  # 'normal' or 'uniform'\n",
        "\n",
        "TARGET_TRANSFORM = 'none'  # 'none'|'log1p'|'standard'\n",
        "\n",
        "def make_feature_transformer(name):\n",
        "    key = (name or 'none').lower()\n",
        "    if key == 'standard':\n",
        "        return StandardScaler()\n",
        "    if key == 'robust':\n",
        "        return RobustScaler()\n",
        "    if key == 'quantile':\n",
        "        return QuantileTransformer(output_distribution=QUANTILE_OUTPUT, random_state=SEED)\n",
        "    if key == 'power':\n",
        "        return PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "    return None\n",
        "\n",
        "def fit_target_transform(y, name):\n",
        "    key = (name or 'none').lower()\n",
        "    yy = pd.Series(y).astype(float)\n",
        "    if key == 'log1p':\n",
        "        return np.log1p(yy).astype(np.float32).values, {'name':'log1p'}\n",
        "    if key == 'standard':\n",
        "        mu = float(yy.mean()); sd = float(yy.std() + 1e-8)\n",
        "        return ((yy - mu)/sd).astype(np.float32).values, {'name':'standard','mean':mu,'std':sd}\n",
        "    return yy.astype(np.float32).values, {'name':'none'}\n",
        "\n",
        "def apply_target_transform(y, params):\n",
        "    key = (params.get('name') or 'none').lower()\n",
        "    yy = pd.Series(y).astype(float)\n",
        "    if key == 'log1p':\n",
        "        return np.log1p(yy).astype(np.float32).values\n",
        "    if key == 'standard':\n",
        "        mu = params['mean']; sd = params['std']\n",
        "        return ((yy - mu)/sd).astype(np.float32).values\n",
        "    return yy.astype(np.float32).values\n",
        "\n",
        "def inverse_target_transform(arr, params):\n",
        "    key = (params.get('name') or 'none').lower()\n",
        "    a = np.asarray(arr, dtype=float)\n",
        "    if key == 'log1p':\n",
        "        return np.expm1(a)\n",
        "    if key == 'standard':\n",
        "        return a * params['std'] + params['mean']\n",
        "    return a\n",
        "\n",
        "# Split first to avoid leakage\n",
        "X_train_raw, X_val_raw, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Impute missing values (fit on train only)\n",
        "imputer = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
        "X_train_imp = imputer.fit_transform(X_train_raw)\n",
        "X_val_imp = imputer.transform(X_val_raw)\n",
        "test_imp = imputer.transform(test)\n",
        "\n",
        "# Feature transform (fit on train only)\n",
        "_ft = make_feature_transformer(FEATURE_TRANSFORM)\n",
        "if _ft is None:\n",
        "    X_train = X_train_imp\n",
        "    X_val = X_val_imp\n",
        "    test_scaled = test_imp\n",
        "else:\n",
        "    X_train = _ft.fit_transform(X_train_imp)\n",
        "    X_val = _ft.transform(X_val_imp)\n",
        "    test_scaled = _ft.transform(test_imp)\n",
        "\n",
        "# Target transform (fit on train only)\n",
        "y_train_t, TARGET_PARAMS_SINGLE = fit_target_transform(y_train, TARGET_TRANSFORM)\n",
        "y_val_t = apply_target_transform(y_val, TARGET_PARAMS_SINGLE)\n",
        "y_train_proc = pd.Series(y_train_t, index=y_train.index)\n",
        "y_val_proc = pd.Series(y_val_t, index=y_val.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:38.640046Z",
          "iopub.status.busy": "2024-10-31T16:06:38.639302Z",
          "iopub.status.idle": "2024-10-31T16:06:38.677768Z",
          "shell.execute_reply": "2024-10-31T16:06:38.676875Z",
          "shell.execute_reply.started": "2024-10-31T16:06:38.639993Z"
        },
        "id": "TaFE3dtqj6fT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class OceanChemistryDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        # y は pandas.Series でも numpy.ndarray でも受け付ける\n",
        "        y_arr = y.values if hasattr(y, 'values') else y\n",
        "        self.y = torch.tensor(y_arr, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = OceanChemistryDataset(X_train, y_train_proc)\n",
        "val_dataset = OceanChemistryDataset(X_val, y_val_proc)\n",
        "\n",
        "# === 調整ポイント: 学習のバッチ設定 ===\n",
        "# ・batch_size: 32/64/128/256 あたりで比較（大きいほど安定・速いが汎化は要CVで検証）→実際１が最強\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:38.679793Z",
          "iopub.status.busy": "2024-10-31T16:06:38.679208Z",
          "iopub.status.idle": "2024-10-31T16:06:38.702210Z",
          "shell.execute_reply": "2024-10-31T16:06:38.701153Z",
          "shell.execute_reply.started": "2024-10-31T16:06:38.679681Z"
        },
        "id": "8kUw3FU7j6fT",
        "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1163, 15)\n"
          ]
        }
      ],
      "source": [
        "# ===== MLPハイパラ（ここを主に調整） =====\n",
        "# DROPOUT_P: 0.1〜0.3 推奨（0で無効）\n",
        "DROPOUT_P = 0  # Dropout probability (0.1-0.3 recommended)\n",
        "# ACTIVATION_NAME: 'ReLU'|'GELU'|'SiLU'|'Tanh'|'LeakyReLU'\n",
        "ACTIVATION_NAME = 'Tanh'  # Options: ReLU, GELU, SiLU, Tanh, LeakyReLU\n",
        "\n",
        "def make_activation(name):\n",
        "    try:\n",
        "        key = (name or 'ReLU').lower()\n",
        "    except Exception:\n",
        "        key = 'relu'\n",
        "    if key == 'relu':\n",
        "        return nn.ReLU()\n",
        "    if key == 'gelu':\n",
        "        return nn.GELU()\n",
        "    if key in ('silu','swish'):\n",
        "        return nn.SiLU()\n",
        "    if key == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    if key in ('leakyrelu','lrelu'):\n",
        "        return nn.LeakyReLU(0.01)\n",
        "    return nn.ReLU()\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME):\n",
        "\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        # 隠れ層ユニット数（今は1024。64〜1024で比較してみてください）\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "\n",
        "        self.act1 = make_activation(activation_name)\n",
        "\n",
        "        self.drop1 = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.fc3 = nn.Linear(1024, 1)  # Output layer for regression\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "model = MLPModel(input_size=X_train.shape[1], dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME)\n",
        "\n",
        "print(X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:38.705593Z",
          "iopub.status.busy": "2024-10-31T16:06:38.705176Z",
          "iopub.status.idle": "2024-10-31T16:09:35.511105Z",
          "shell.execute_reply": "2024-10-31T16:09:35.509987Z",
          "shell.execute_reply.started": "2024-10-31T16:06:38.705553Z"
        },
        "id": "TZrodbLrj6fT",
        "outputId": "15a041e0-6c82-455e-c55a-55bd117814ed",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5000, Train Loss: 4633887.947368421, Validation Loss: 4612593.2\n",
            "Epoch 101/5000, Train Loss: 15084.370579769737, Validation Loss: 18933.8005859375\n",
            "Epoch 201/5000, Train Loss: 2547.9099185341283, Validation Loss: 3594.0954345703126\n",
            "Epoch 301/5000, Train Loss: 716.5324530350534, Validation Loss: 948.5962158203125\n",
            "Epoch 401/5000, Train Loss: 400.37651142321135, Validation Loss: 575.134521484375\n",
            "Epoch 501/5000, Train Loss: 284.9509711014597, Validation Loss: 413.65491943359376\n",
            "Epoch 601/5000, Train Loss: 157.120555676912, Validation Loss: 215.35331115722656\n",
            "Epoch 701/5000, Train Loss: 115.85318153782895, Validation Loss: 136.43557891845703\n",
            "Epoch 801/5000, Train Loss: 78.71800462823165, Validation Loss: 102.47501983642579\n",
            "Epoch 901/5000, Train Loss: 79.39923437018143, Validation Loss: 84.50692291259766\n",
            "Epoch 1001/5000, Train Loss: 61.57915757831774, Validation Loss: 90.80420532226563\n",
            "Epoch 1101/5000, Train Loss: 59.011202460841126, Validation Loss: 79.94369735717774\n",
            "Epoch 1201/5000, Train Loss: 44.262922738727774, Validation Loss: 54.60122985839844\n",
            "Epoch 1301/5000, Train Loss: 45.66606300755551, Validation Loss: 71.30249328613282\n",
            "Epoch 1401/5000, Train Loss: 39.66262847498844, Validation Loss: 62.98748016357422\n",
            "Epoch 1501/5000, Train Loss: 32.760126063698216, Validation Loss: 51.929491424560545\n",
            "Epoch 1601/5000, Train Loss: 34.770258250989414, Validation Loss: 45.674285125732425\n",
            "Epoch 1701/5000, Train Loss: 34.51503743623432, Validation Loss: 46.07814178466797\n",
            "Epoch 1801/5000, Train Loss: 43.35380002071983, Validation Loss: 48.40091400146484\n",
            "Epoch 1901/5000, Train Loss: 34.933612622712786, Validation Loss: 122.43202896118164\n",
            "Epoch 2001/5000, Train Loss: 34.670268761484245, Validation Loss: 72.05145797729492\n",
            "Epoch 2101/5000, Train Loss: 35.82535342166298, Validation Loss: 44.939079666137694\n",
            "Epoch 2201/5000, Train Loss: 37.402690636484245, Validation Loss: 73.06920852661133\n",
            "Epoch 2301/5000, Train Loss: 43.179393667923776, Validation Loss: 64.67569885253906\n",
            "Epoch 2401/5000, Train Loss: 26.058987617492676, Validation Loss: 45.704331970214845\n",
            "Epoch 2501/5000, Train Loss: 27.63914273914538, Validation Loss: 44.29252738952637\n",
            "Epoch 2601/5000, Train Loss: 27.794708753886976, Validation Loss: 59.06520156860351\n",
            "Epoch 2701/5000, Train Loss: 31.337925659982783, Validation Loss: 71.96796264648438\n",
            "Epoch 2801/5000, Train Loss: 22.820767503035697, Validation Loss: 35.30337066650391\n",
            "Epoch 2901/5000, Train Loss: 27.68800253617136, Validation Loss: 49.846096420288085\n",
            "Epoch 3001/5000, Train Loss: 60.380249675951504, Validation Loss: 59.2910717010498\n",
            "Epoch 3101/5000, Train Loss: 36.74450937070345, Validation Loss: 43.3547061920166\n",
            "Epoch 3201/5000, Train Loss: 30.514942068802682, Validation Loss: 38.17370433807373\n",
            "Epoch 3301/5000, Train Loss: 23.526791999214574, Validation Loss: 39.81262474060058\n",
            "Epoch 3401/5000, Train Loss: 26.541675567626953, Validation Loss: 46.78069152832031\n",
            "Epoch 3501/5000, Train Loss: 30.57578518516139, Validation Loss: 43.97907066345215\n",
            "Epoch 3601/5000, Train Loss: 25.92467097232216, Validation Loss: 47.50724105834961\n",
            "Epoch 3701/5000, Train Loss: 26.10161946949206, Validation Loss: 45.497413063049315\n",
            "Epoch 3801/5000, Train Loss: 21.197749087685033, Validation Loss: 44.4531192779541\n",
            "Epoch 3901/5000, Train Loss: 23.96062886087518, Validation Loss: 38.40642166137695\n",
            "Epoch 4001/5000, Train Loss: 21.920030719355534, Validation Loss: 43.43920211791992\n",
            "Epoch 4101/5000, Train Loss: 25.60328365627088, Validation Loss: 43.016561889648436\n",
            "Epoch 4201/5000, Train Loss: 21.379806066814222, Validation Loss: 46.96633796691894\n",
            "Epoch 4301/5000, Train Loss: 20.608908954419586, Validation Loss: 47.01842346191406\n",
            "Epoch 4401/5000, Train Loss: 28.0404204067431, Validation Loss: 42.011796951293945\n",
            "Epoch 4501/5000, Train Loss: 23.913236266688298, Validation Loss: 45.37639617919922\n",
            "Epoch 4601/5000, Train Loss: 29.726560065620824, Validation Loss: 117.87635116577148\n",
            "Epoch 4701/5000, Train Loss: 19.73130635211342, Validation Loss: 40.123684883117676\n",
            "Epoch 4801/5000, Train Loss: 18.472513612947967, Validation Loss: 41.08392143249512\n",
            "Epoch 4901/5000, Train Loss: 19.049142159913714, Validation Loss: 40.933606719970705\n"
          ]
        }
      ],
      "source": [
        "WEIGHT_DECAY = 1e-4  # 1e-4 to 1e-3 recommended\n",
        "EARLY_STOPPING_PATIENCE = 200  # epochs with no improvement before stop\n",
        "EARLY_STOPPING_MIN_DELTA = 1e-4  # minimum improvement to reset patience\n",
        "\n",
        "# Model selection toggle: 'BP' (PyTorch backprop), 'ELM', 'RBF'\n",
        "MODEL_NAME = 'BP'\n",
        "\n",
        "# ELM hyperparameters\n",
        "ELM_HIDDEN = 512\n",
        "ELM_REG = 1e-2  # ridge regularization\n",
        "ELM_ACTIVATION = 'relu'  # relu|tanh|sigmoid\n",
        "\n",
        "# RBF hyperparameters\n",
        "RBF_UNITS = 100\n",
        "RBF_REG = 1e-2\n",
        "RBF_SIGMA_SCALE = 1.0  # scale factor for sigma derived from centers\n",
        "\n",
        "import torch.optim as optim\n",
        "# ===== 最適化/損失/早期終了（主な調整ポイント） =====\n",
        "# OPTIMIZER_NAME: 'Adam'|'AdamW'|'SGD'|'RMSprop'|'Adagrad'\n",
        "#   ・SGDを使うなら OPTIMIZER_PARAMS={'momentum':0.9,'nesterov':True} など\n",
        "# WEIGHT_DECAY: 1e-4〜1e-3 推奨（L2正則化。大きすぎると学習が弱まる）\n",
        "# LOSS_NAME: 'SmoothL1'|'MSE'|'L1'|'Huber'（Huberはdelta、SmoothL1はbetaをLOSS_PARAMSで指定可）\n",
        "# EARLY_STOPPING_PATIENCE/MIN_DELTA: 早期終了の判定\n",
        "# 学習率lrはmake_optimizerの引数で指定。ReduceLROnPlateauで自動減衰\n",
        "\n",
        "# Optimizer toggle\n",
        "OPTIMIZER_NAME = 'AdamW'  # Options: 'Adam','AdamW','SGD','RMSprop','Adagrad'\n",
        "OPTIMIZER_PARAMS = {}  # e.g., {'momentum':0.9} for SGD\n",
        "\n",
        "def make_optimizer(name, params, **kwargs):\n",
        "    try:\n",
        "        key = (name or 'Adam').lower()\n",
        "    except Exception:\n",
        "        key = 'adam'\n",
        "    lr = kwargs.get('lr', 1e-3)\n",
        "    wd = kwargs.get('weight_decay', 0.0)\n",
        "    if key == 'adamw':\n",
        "        return optim.AdamW(params, lr=lr, weight_decay=wd)\n",
        "    if key == 'sgd':\n",
        "        return optim.SGD(params, lr=lr, momentum=kwargs.get('momentum', 0.9), nesterov=kwargs.get('nesterov', False), weight_decay=wd)\n",
        "    if key == 'rmsprop':\n",
        "        return optim.RMSprop(params, lr=lr, momentum=kwargs.get('momentum', 0.0), alpha=kwargs.get('alpha', 0.99), weight_decay=wd)\n",
        "    if key == 'adagrad':\n",
        "        return optim.Adagrad(params, lr=lr, weight_decay=wd)\n",
        "    return optim.Adam(params, lr=lr, weight_decay=wd)\n",
        "\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "\n",
        "LOSS_NAME = 'SmoothL1'  # Options: 'SmoothL1', 'MSE', 'L1', 'Huber'\n",
        "LOSS_PARAMS = {}  # e.g., {'beta': 1.0} for SmoothL1 or {'delta': 1.0} for Huber\n",
        "\n",
        "def make_loss(name, **kwargs):\n",
        "    try:\n",
        "        key = (name or 'SmoothL1').lower()\n",
        "    except Exception:\n",
        "        key = 'smoothl1'\n",
        "    if key in ('mse','mseloss'):\n",
        "        return nn.MSELoss()\n",
        "    if key in ('l1','mae','l1loss'):\n",
        "        return nn.L1Loss()\n",
        "    if key in ('huber','huberloss'):\n",
        "        delta = kwargs.get('delta', 1.0)\n",
        "        try:\n",
        "            return nn.HuberLoss(delta=delta)\n",
        "        except TypeError:\n",
        "            return nn.SmoothL1Loss()\n",
        "    # Default SmoothL1\n",
        "    beta = kwargs.get('beta', 1.0)\n",
        "    try:\n",
        "        return nn.SmoothL1Loss(beta=beta)\n",
        "    except TypeError:\n",
        "        return nn.SmoothL1Loss()\n",
        "\n",
        "\n",
        "criterion = make_loss(LOSS_NAME, **LOSS_PARAMS)\n",
        "\n",
        "optimizer = make_optimizer(OPTIMIZER_NAME, model.parameters(), lr=0.001, weight_decay=WEIGHT_DECAY, **OPTIMIZER_PARAMS)\n",
        "\n",
        "# LR scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6)\n",
        "\n",
        "\n",
        "def _np_activation(name, X):\n",
        "    key = (name or 'relu').lower()\n",
        "    if key == 'relu':\n",
        "        return (X > 0) * X\n",
        "    if key == 'tanh':\n",
        "        return np.tanh(X)\n",
        "    if key in ('sigmoid','logistic'):\n",
        "        return 1.0 / (1.0 + np.exp(-X))\n",
        "    return (X > 0) * X\n",
        "\n",
        "def _ridge_solve(H, y, reg):\n",
        "    # Solve (H^T H + reg I) w = H^T y\n",
        "    HtH = H.T @ H\n",
        "    n = HtH.shape[0]\n",
        "    A = HtH + reg * np.eye(n)\n",
        "    b = H.T @ y\n",
        "    return np.linalg.solve(A, b)\n",
        "\n",
        "def fit_elm(X, y, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=42):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    W = rs.normal(scale=1.0, size=(X.shape[1], hidden))\n",
        "    b = rs.normal(scale=1.0, size=(hidden,))\n",
        "    H = _np_activation(act, X @ W + b)\n",
        "    beta = _ridge_solve(H, y.astype(float), reg)\n",
        "    return {'W': W, 'b': b, 'beta': beta, 'act': act}\n",
        "\n",
        "def predict_elm(model_dict, X):\n",
        "    W = model_dict['W']; b = model_dict['b']; act = model_dict['act']; beta = model_dict['beta']\n",
        "    H = _np_activation(act, X @ W + b)\n",
        "    return H @ beta\n",
        "\n",
        "def _rbf_design(X, centers, gamma):\n",
        "    # Compute squared Euclidean distances efficiently\n",
        "    X2 = np.sum(X*X, axis=1, keepdims=True)\n",
        "    C2 = np.sum(centers*centers, axis=1)[None, :]\n",
        "    dist2 = X2 + C2 - 2.0 * (X @ centers.T)\n",
        "    return np.exp(-gamma * dist2)\n",
        "\n",
        "def fit_rbf(X, y, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=42):\n",
        "    from sklearn.cluster import KMeans\n",
        "    km = KMeans(n_clusters=units, random_state=seed, n_init=10)\n",
        "    centers = km.fit(X).cluster_centers_\n",
        "    # Estimate sigma from center distances\n",
        "    from scipy.spatial.distance import cdist\n",
        "    try:\n",
        "        import numpy as _np\n",
        "        pair = _np.linalg.norm(centers[:,None,:]-centers[None,:,:], axis=2)\n",
        "    except Exception:\n",
        "        pair = np.zeros((units, units))\n",
        "    # Use median of nearest-neighbor distances\n",
        "    nn = []\n",
        "    for i in range(units):\n",
        "        vals = np.sort(pair[i][pair[i]>0])\n",
        "        if vals.size>0:\n",
        "            nn.append(vals[0])\n",
        "    sigma = (np.median(nn) if len(nn)>0 else 1.0) * sigma_scale\n",
        "    sigma = max(sigma, 1e-6)\n",
        "    gamma = 1.0/(2.0*sigma*sigma)\n",
        "    Phi = _rbf_design(X, centers, gamma)\n",
        "    w = _ridge_solve(Phi, y.astype(float), reg)\n",
        "    return {'centers': centers, 'gamma': gamma, 'w': w}\n",
        "\n",
        "def predict_rbf(model_dict, X):\n",
        "    centers = model_dict['centers']; gamma = model_dict['gamma']; w = model_dict['w']\n",
        "    Phi = _rbf_design(X, centers, gamma)\n",
        "    return Phi @ w\n",
        "\n",
        "\n",
        "# Training function\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5000, scheduler=None):\n",
        "\n",
        "    import copy\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "\n",
        "            loss = criterion(outputs.squeeze(), y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        val_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for X_batch, y_batch in val_loader:\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "\n",
        "                loss = criterion(outputs.squeeze(), y_batch)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss_avg = val_loss/len(val_loader) if len(val_loader)>0 else val_loss\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss_avg)\n",
        "\n",
        "        # Early stopping (only if we have validation batches)\n",
        "        if len(val_loader) > 0:\n",
        "            if best_val - val_loss_avg > EARLY_STOPPING_MIN_DELTA:\n",
        "                best_val = val_loss_avg\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch+1}; best val: {best_val:.6f}\")\n",
        "                if best_state is not None:\n",
        "                    model.load_state_dict(best_state)\n",
        "                break\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, LR: {current_lr:.2e}, Train Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss_avg}\")\n",
        "\n",
        "    # Load best state at the end if available\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "# Train based on selected model\n",
        "\n",
        "if MODEL_NAME.upper() == 'BP':\n",
        "    train_model(model, train_loader, val_loader, epochs=5000, scheduler=scheduler)\n",
        "elif MODEL_NAME.upper() == 'ELM':\n",
        "    print('Training ELM...')\n",
        "    ELM_MODEL = fit_elm(X_train, y_train, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=SEED)\n",
        "elif MODEL_NAME.upper() == 'RBF':\n",
        "    print('Training RBF...')\n",
        "    RBF_MODEL = fit_rbf(X_train, y_train, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=SEED)\n",
        "else:\n",
        "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:09:35.513910Z",
          "iopub.status.busy": "2024-10-31T16:09:35.512949Z",
          "iopub.status.idle": "2024-10-31T16:09:35.534684Z",
          "shell.execute_reply": "2024-10-31T16:09:35.533508Z",
          "shell.execute_reply.started": "2024-10-31T16:09:35.513841Z"
        },
        "id": "CD3YH6Drj6fT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Predict according to MODEL_NAME\n",
        "if MODEL_NAME.upper() == 'BP':\n",
        "    # Convert the test set into a torch tensor\n",
        "    test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_tensor).squeeze().numpy()\n",
        "elif MODEL_NAME.upper() == 'ELM':\n",
        "    predictions = predict_elm(ELM_MODEL, test_scaled)\n",
        "elif MODEL_NAME.upper() == 'RBF':\n",
        "    predictions = predict_rbf(RBF_MODEL, test_scaled)\n",
        "else:\n",
        "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n",
        "\n",
        "# Inverse target transform if applied (single split)\n",
        "try:\n",
        "    predictions = inverse_target_transform(predictions, TARGET_PARAMS_SINGLE)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Prepare submission\n",
        "submission = pd.DataFrame({\"id\": range(1455, 1455 + len(predictions)), \"DIC\": predictions})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:29:07.659023Z",
          "iopub.status.busy": "2024-10-31T16:29:07.657580Z",
          "iopub.status.idle": "2024-10-31T16:29:08.802653Z",
          "shell.execute_reply": "2024-10-31T16:29:08.801353Z",
          "shell.execute_reply.started": "2024-10-31T16:29:07.658961Z"
        },
        "id": "PT0FYnYoj6fU",
        "outputId": "d759f11e-b01d-494c-eddf-a4e051a94198",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.14\n"
          ]
        }
      ],
      "source": [
        "# score: 5.98852\n",
        "# name: 杉浦孔明\n",
        "# id: 123456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfold_cell"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import PowerTransformer, StandardScaler, RobustScaler, QuantileTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# K-Fold cross-validation (reduce seed dependence, avoid leakage)\n",
        "# === CVの設定（調整ポイント） ===\n",
        "n_splits = 5  # 5 or 10 が定番\n",
        "# 注: データ順の偏りを避けたい場合は shuffle=True, random_state=SEED を検討\n",
        "kf = KFold(n_splits=n_splits, shuffle=False)\n",
        "\n",
        "test_pred_sum = np.zeros(len(test))\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(kf.split(X), 1):\n",
        "    # Split raw features\n",
        "    X_train_raw, X_val_raw = X.iloc[trn_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
        "\n",
        "    # Impute and transform per fold (fit only on training)\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_imp = imputer.fit_transform(X_train_raw)\n",
        "    X_val_imp = imputer.transform(X_val_raw)\n",
        "    test_imp = imputer.transform(test)\n",
        "\n",
        "    # Feature transform (fit on train only)\n",
        "    _ft = make_feature_transformer(FEATURE_TRANSFORM)\n",
        "    if _ft is None:\n",
        "        X_train_scaled = X_train_imp\n",
        "        X_val_scaled = X_val_imp\n",
        "        test_scaled_fold = test_imp\n",
        "    else:\n",
        "        X_train_scaled = _ft.fit_transform(X_train_imp)\n",
        "        X_val_scaled = _ft.transform(X_val_imp)\n",
        "        test_scaled_fold = _ft.transform(test_imp)\n",
        "\n",
        "    # Target transform per fold\n",
        "    y_train_t, _tparams = fit_target_transform(y_train, TARGET_TRANSFORM)\n",
        "    y_val_t = apply_target_transform(y_val, _tparams)\n",
        "    print(f'Fold {fold}/{n_splits}')\n",
        "    if MODEL_NAME.upper() == 'BP':\n",
        "        train_dataset = OceanChemistryDataset(X_train_scaled, y_train_t)\n",
        "        val_dataset = OceanChemistryDataset(X_val_scaled, y_val_t)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "        model = MLPModel(input_size=X.shape[1], dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME)\n",
        "        optimizer = make_optimizer(OPTIMIZER_NAME, model.parameters(), lr=0.001, weight_decay=WEIGHT_DECAY, **OPTIMIZER_PARAMS)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6)\n",
        "        train_model(model, train_loader, val_loader, epochs=5000, scheduler=scheduler)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            fold_pred_t = model(torch.tensor(test_scaled_fold, dtype=torch.float32)).squeeze().numpy()\n",
        "    elif MODEL_NAME.upper() == 'ELM':\n",
        "        ELM_FOLD = fit_elm(X_train_scaled, y_train_t, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=SEED+fold)\n",
        "        fold_pred = predict_elm(ELM_FOLD, test_scaled_fold)\n",
        "    elif MODEL_NAME.upper() == 'RBF':\n",
        "        RBF_FOLD = fit_rbf(X_train_scaled, y_train_t, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=SEED+fold)\n",
        "        fold_pred = predict_rbf(RBF_FOLD, test_scaled_fold)\n",
        "    else:\n",
        "        raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n",
        "\n",
        "\n",
        "    # Inverse target transform back to original scale\n",
        "    try:\n",
        "        fold_pred = inverse_target_transform(locals().get('fold_pred_t', locals().get('fold_pred')), _tparams)\n",
        "    except Exception:\n",
        "        fold_pred = locals().get('fold_pred_t', locals().get('fold_pred'))\n",
        "    test_pred_sum += fold_pred\n",
        "\n",
        "# Averaged predictions across folds\n",
        "predictions = test_pred_sum / n_splits\n",
        "\n",
        "# Prepare submission (overwrites with K-Fold average)\n",
        "submission = pd.DataFrame({'id': range(1455, 1455 + len(predictions)), 'DIC': predictions})\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "optuna_nn_cell"
      },
      "outputs": [],
      "source": [
        "# Optuna hyperparameter optimization for NN (K-Fold CV)\n",
        "try:\n",
        "    import optuna\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'optuna'])\n",
        "    import optuna\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Optuna settings\n",
        "NN_N_TRIALS = 30\n",
        "NN_N_EPOCHS = 1500\n",
        "NN_KFOLDS = 5\n",
        "\n",
        "def train_model_local(model, train_loader, val_loader, epochs, optimizer, criterion, scheduler=None, patience=EARLY_STOPPING_PATIENCE, min_delta=EARLY_STOPPING_MIN_DELTA, trial=None):\n",
        "    import copy\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = criterion(out.squeeze(), yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item()\n",
        "        # val\n",
        "        model.eval()\n",
        "        vloss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                out = model(xb)\n",
        "                loss = criterion(out.squeeze(), yb)\n",
        "                vloss += loss.item()\n",
        "        vloss_avg = vloss/len(val_loader) if len(val_loader)>0 else vloss\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(vloss_avg)\n",
        "        # early stop\n",
        "        if len(val_loader) > 0:\n",
        "            if best_val - vloss_avg > min_delta:\n",
        "                best_val = vloss_avg\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "            if trial is not None:\n",
        "                trial.report(vloss_avg, epoch)\n",
        "                if trial.should_prune():\n",
        "                    raise optuna.TrialPruned()\n",
        "            if epochs_no_improve >= patience:\n",
        "                if best_state is not None:\n",
        "                    model.load_state_dict(best_state)\n",
        "                break\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return best_val\n",
        "\n",
        "def objective(trial):\n",
        "    # search space\n",
        "    dropout_p = trial.suggest_float('dropout_p', 0.0, 0.4)\n",
        "    activation = trial.suggest_categorical('activation', ['ReLU','GELU','SiLU','Tanh','LeakyReLU'])\n",
        "    lr = trial.suggest_float('lr', 1e-4, 3e-3, log=True)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "    loss_name = trial.suggest_categorical('loss', ['SmoothL1','MSE','Huber'])\n",
        "    loss_params = {}\n",
        "    if loss_name == 'Huber':\n",
        "        loss_params['delta'] = trial.suggest_float('huber_delta', 0.5, 5.0, log=True)\n",
        "    if loss_name == 'SmoothL1':\n",
        "        loss_params['beta'] = trial.suggest_float('smoothl1_beta', 0.5, 2.0)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
        "\n",
        "    kf = KFold(n_splits=NN_KFOLDS, shuffle=True, random_state=SEED)\n",
        "    fold_losses = []\n",
        "    for fold, (trn_idx, val_idx) in enumerate(kf.split(X), 1):\n",
        "        X_tr_raw, X_va_raw = X.iloc[trn_idx], X.iloc[val_idx]\n",
        "        y_tr, y_va = y.iloc[trn_idx], y.iloc[val_idx]\n",
        "        # impute/transform per fold\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X_tr_imp = imputer.fit_transform(X_tr_raw)\n",
        "        X_va_imp = imputer.transform(X_va_raw)\n",
        "        pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "        X_tr = pt.fit_transform(X_tr_imp)\n",
        "        X_va = pt.transform(X_va_imp)\n",
        "        # dataset/loader\n",
        "        train_dataset = OceanChemistryDataset(X_tr, y_tr)\n",
        "        val_dataset = OceanChemistryDataset(X_va, y_va)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        # model/opt/loss\n",
        "        model = MLPModel(input_size=X.shape[1], dropout_p=dropout_p, activation_name=activation)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        # reuse helper from training cell if present\n",
        "        try:\n",
        "            criterion = make_loss(loss_name, **loss_params)\n",
        "        except Exception:\n",
        "            # fallback\n",
        "            if loss_name == 'MSE':\n",
        "                criterion = nn.MSELoss()\n",
        "            elif loss_name == 'Huber':\n",
        "                try:\n",
        "                    criterion = nn.HuberLoss(delta=loss_params.get('delta', 1.0))\n",
        "                except Exception:\n",
        "                    criterion = nn.SmoothL1Loss()\n",
        "            else:\n",
        "                try:\n",
        "                    criterion = nn.SmoothL1Loss(beta=loss_params.get('beta', 1.0))\n",
        "                except Exception:\n",
        "                    criterion = nn.SmoothL1Loss()\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6)\n",
        "        # train\n",
        "        vloss = train_model_local(model, train_loader, val_loader, epochs=NN_N_EPOCHS, optimizer=optimizer, criterion=criterion, scheduler=scheduler, patience=EARLY_STOPPING_PATIENCE, min_delta=EARLY_STOPPING_MIN_DELTA, trial=trial)\n",
        "        fold_losses.append(vloss)\n",
        "    return float(np.mean(fold_losses))\n",
        "\n",
        "pruner = optuna.pruners.MedianPruner(n_warmup_steps=50)\n",
        "study = optuna.create_study(direction='minimize', pruner=pruner, study_name='nn_kfold_rmse')\n",
        "study.optimize(objective, n_trials=NN_N_TRIALS, gc_after_trial=True)\n",
        "print('Best value (val loss):', study.best_value)\n",
        "print('Best params:', study.best_params)\n",
        "\n",
        "# Optional: train best params on K-Fold and save submission\n",
        "TRAIN_BEST_AND_SUBMIT = False\n",
        "if TRAIN_BEST_AND_SUBMIT:\n",
        "    best = study.best_params\n",
        "    dropout_p = best.get('dropout_p', 0.0)\n",
        "    activation = best.get('activation', 'ReLU')\n",
        "    lr = best.get('lr', 1e-3)\n",
        "    weight_decay = best.get('weight_decay', 0.0)\n",
        "    loss_name = best.get('loss', 'SmoothL1')\n",
        "    loss_params = {}\n",
        "    if loss_name == 'Huber':\n",
        "        loss_params['delta'] = best.get('huber_delta', 1.0)\n",
        "    if loss_name == 'SmoothL1':\n",
        "        loss_params['beta'] = best.get('smoothl1_beta', 1.0)\n",
        "    batch_size = best.get('batch_size', 64)\n",
        "    kf = KFold(n_splits=NN_KFOLDS, shuffle=True, random_state=SEED)\n",
        "    test_pred_sum = np.zeros(len(test))\n",
        "    for fold, (trn_idx, val_idx) in enumerate(kf.split(X), 1):\n",
        "        X_tr_raw, X_va_raw = X.iloc[trn_idx], X.iloc[val_idx]\n",
        "        y_tr, y_va = y.iloc[trn_idx], y.iloc[val_idx]\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X_tr_imp = imputer.fit_transform(X_tr_raw)\n",
        "        X_va_imp = imputer.transform(X_va_raw)\n",
        "        test_imp = imputer.transform(test)\n",
        "        pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "        X_tr = pt.fit_transform(X_tr_imp)\n",
        "        X_va = pt.transform(X_va_imp)\n",
        "        test_fold = pt.transform(test_imp)\n",
        "        train_dataset = OceanChemistryDataset(X_tr, y_tr)\n",
        "        val_dataset = OceanChemistryDataset(X_va, y_va)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        model = MLPModel(input_size=X.shape[1], dropout_p=dropout_p, activation_name=activation)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        try:\n",
        "            criterion = make_loss(loss_name, **loss_params)\n",
        "        except Exception:\n",
        "            criterion = nn.SmoothL1Loss()\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6)\n",
        "        _ = train_model_local(model, train_loader, val_loader, epochs=NN_N_EPOCHS, optimizer=optimizer, criterion=criterion, scheduler=scheduler, patience=EARLY_STOPPING_PATIENCE, min_delta=EARLY_STOPPING_MIN_DELTA, trial=None)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_pred_sum += model(torch.tensor(test_fold, dtype=torch.float32)).squeeze().numpy()\n",
        "    predictions_optuna = test_pred_sum / NN_KFOLDS\n",
        "    sub_optuna = sub.copy() if 'sub' in globals() else pd.DataFrame({'id': range(1455, 1455 + len(predictions_optuna))})\n",
        "    sub_optuna['DIC'] = predictions_optuna\n",
        "    sub_optuna.to_csv('submission_optuna.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7941427,
          "sourceId": 49552,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
