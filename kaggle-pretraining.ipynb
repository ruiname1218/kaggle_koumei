{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T14:15:30.344162Z",
     "iopub.status.busy": "2025-11-16T14:15:30.343431Z",
     "iopub.status.idle": "2025-11-16T14:15:30.351201Z",
     "shell.execute_reply": "2025-11-16T14:15:30.350685Z",
     "shell.execute_reply.started": "2025-11-16T14:15:30.344134Z"
    },
    "id": "seed_cell"
   },
   "outputs": [],
   "source": [
    "SEED = 8000\n",
    "import os, random, numpy as np\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# PyTorch seeding (TensorFlow is skipped to avoid environment issues)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-16T14:15:32.937095Z",
     "iopub.status.busy": "2025-11-16T14:15:32.936360Z",
     "iopub.status.idle": "2025-11-16T14:15:43.113555Z",
     "shell.execute_reply": "2025-11-16T14:15:43.112731Z",
     "shell.execute_reply.started": "2025-11-16T14:15:32.937067Z"
    },
    "id": "w0miHB_4j6fO",
    "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn<1.7,>=1.2\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.7,>=1.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.7,>=1.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.7,>=1.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn<1.7,>=1.2) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn<1.7,>=1.2) (2024.2.0)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.7.2\n",
      "    Uninstalling scikit-learn-1.7.2:\n",
      "      Successfully uninstalled scikit-learn-1.7.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tabpfn in /usr/local/lib/python3.11/dist-packages (6.0.6)\n",
      "Requirement already satisfied: torch<3,>=2.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy<3,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn<1.8,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.6.1)\n",
      "Requirement already satisfied: typing_extensions>=4.12.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (4.15.0)\n",
      "Requirement already satisfied: scipy<2,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.15.3)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.2.3)\n",
      "Requirement already satisfied: einops<0.9,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub<2,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.1.4)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.12.4)\n",
      "Requirement already satisfied: pydantic-settings>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.11.0)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.5.2)\n",
      "Requirement already satisfied: tabpfn-common-utils>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (0.2.9)\n",
      "Requirement already satisfied: kditransform>=1.2 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (0.20.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2,>=0.19.0->tabpfn) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2,>=0.19.0->tabpfn) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2,>=0.19.0->tabpfn) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2,>=0.19.0->tabpfn) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2,>=0.19.0->tabpfn) (0.16.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.8,>=1.2.0->tabpfn) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.1->tabpfn) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.48 in /usr/local/lib/python3.11/dist-packages (from kditransform>=1.2->tabpfn) (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.48->kditransform>=1.2->tabpfn) (0.43.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings>=2.10.1->tabpfn) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn) (1.17.0)\n",
      "Requirement already satisfied: platformdirs>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (4.5.0)\n",
      "Requirement already satisfied: posthog~=6.7 in /usr/local/lib/python3.11/dist-packages (from tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (6.9.3)\n",
      "Requirement already satisfied: requests>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (2.32.5)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog~=6.7->tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog~=6.7->tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.5->tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.5->tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2,>=0.19.0->tabpfn) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.1->tabpfn) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21.6->tabpfn) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21.6->tabpfn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21.6->tabpfn) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.21.6->tabpfn) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.21.6->tabpfn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.21.6->tabpfn) (2024.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub<2,>=0.19.0->tabpfn) (8.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\n",
      "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\n",
      "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pip setuptools wheel\n",
    "%pip install 'scikit-learn>=1.2,<1.7'\n",
    "%pip install -U tabpfn\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:35:53.029120Z",
     "iopub.status.busy": "2025-11-16T13:35:53.028849Z",
     "iopub.status.idle": "2025-11-16T13:35:53.048812Z",
     "shell.execute_reply": "2025-11-16T13:35:53.048124Z",
     "shell.execute_reply.started": "2025-11-16T13:35:53.029098Z"
    },
    "id": "wmcUA-Kyj6fR"
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tabpfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T14:15:15.967438Z",
     "iopub.status.busy": "2025-11-16T14:15:15.966749Z",
     "iopub.status.idle": "2025-11-16T14:15:18.375917Z",
     "shell.execute_reply": "2025-11-16T14:15:18.374736Z",
     "shell.execute_reply.started": "2025-11-16T14:15:15.967414Z"
    },
    "id": "2SYojsJUj6fS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (1.1.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface_hub) (8.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 12', 'id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/1258331644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Drop unneeded columns and handle missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Unnamed: 12\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Dropping unnecessary columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Find common columns between train and test (excluding \"DIC\" from train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Unnamed: 12', 'id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler, QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pip install -U huggingface_hub\n",
    "from huggingface_hub import login; login(\"\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "# train = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "# test = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "\n",
    "# Drop unneeded columns and handle missing values\n",
    "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
    "\n",
    "# Find common columns between train and test (excluding \"DIC\" from train)\n",
    "common_columns = train.drop(columns=[\"DIC\"]).columns.intersection(test.columns)\n",
    "\n",
    "# Select the common columns for both train and test\n",
    "X = train[common_columns].copy()\n",
    "y = train[\"DIC\"]\n",
    "test = test[common_columns].copy()\n",
    "\n",
    "\n",
    "# Toggle for feature engineering (set to False to disable)\n",
    "# === 調整ポイント: 特徴量ENGのON/OFF（効果比較に便利） ===\n",
    "# True: 追加特徴（N_tot, 比率, 交互作用, sin/cos 等）を使う\n",
    "# False: 生の共通列のみで学習\n",
    "FE_ENABLED = False\n",
    "\n",
    "if FE_ENABLED:\n",
    "    # Feature engineering (deterministic; same for train/test)\n",
    "    # Totals and ratios (avoid divide-by-zero -> NaN; imputer will handle)\n",
    "    X['N_tot'] = X['NO3uM'] + X['NO2uM'] + X['NH3uM']\n",
    "    test['N_tot'] = test['NO3uM'] + test['NO2uM'] + test['NH3uM']\n",
    "    X['N_to_P'] = np.where(X['PO4uM'] == 0, np.nan, X['NO3uM'] / X['PO4uM'])\n",
    "    test['N_to_P'] = np.where(test['PO4uM'] == 0, np.nan, test['NO3uM'] / test['PO4uM'])\n",
    "    X['Si_to_N'] = np.where(X['NO3uM'] == 0, np.nan, X['SiO3uM'] / X['NO3uM'])\n",
    "    test['Si_to_N'] = np.where(test['NO3uM'] == 0, np.nan, test['SiO3uM'] / test['NO3uM'])\n",
    "    # Interactions\n",
    "    X['Depth_Temp'] = X['R_Depth'] * X['R_TEMP']\n",
    "    test['Depth_Temp'] = test['R_Depth'] * test['R_TEMP']\n",
    "    X['Sal_Temp'] = X['R_Sal'] * X['R_TEMP']\n",
    "    test['Sal_Temp'] = test['R_Sal'] * test['R_TEMP']\n",
    "    # Geographic trig features\n",
    "    X['sin_lat'] = np.sin(np.radians(X['Lat_Dec']))\n",
    "    X['cos_lat'] = np.cos(np.radians(X['Lat_Dec']))\n",
    "    X['sin_lon'] = np.sin(np.radians(X['Lon_Dec']))\n",
    "    X['cos_lon'] = np.cos(np.radians(X['Lon_Dec']))\n",
    "    test['sin_lat'] = np.sin(np.radians(test['Lat_Dec']))\n",
    "    test['cos_lat'] = np.cos(np.radians(test['Lat_Dec']))\n",
    "    test['sin_lon'] = np.sin(np.radians(test['Lon_Dec']))\n",
    "    test['cos_lon'] = np.cos(np.radians(test['Lon_Dec']))\n",
    "    # Replace infs with NaN to be imputed\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# ===== 調整ポイント: 前処理/変換スイッチ =====\n",
    "# FEATURE_IMPUTER: 欠損補完の方法 ('mean' または 'median')\n",
    "# FEATURE_TRANSFORM: 特徴量変換\n",
    "#   - 'power'    : Yeo-Johnson + 標準化（0/負値OK, 初手におすすめ）\n",
    "#   - 'standard' : 標準化のみ（平均0・分散1）\n",
    "#   - 'robust'   : 外れ値に頑健なスケーリング\n",
    "#   - 'quantile' : 分位変換（出力分布は下のQUANTILE_OUTPUTで指定）\n",
    "#   - 'none'     : 変換しない\n",
    "# TARGET_TRANSFORM: 目的変数変換 ('none'|'log1p'|'standard')\n",
    "#   - 'log1p'    : スケール/外れ値を抑制。予測は自動でexpm1逆変換\n",
    "#   - 'standard' : 平均0・分散1に正規化（予測は平均・分散で戻す）\n",
    "# 変更後は前処理→学習→推論の順に実行してください\n",
    "# Switches for feature/target transforms\n",
    "FEATURE_IMPUTER = 'mean'  # 'mean' or 'median'\n",
    "FEATURE_TRANSFORM = 'power'  # 'power'|'standard'|'robust'|'quantile'|'none'\n",
    "QUANTILE_OUTPUT = 'normal'  # 'normal' or 'uniform'\n",
    "\n",
    "TARGET_TRANSFORM = 'none'  # 'none'|'log1p'|'standard'\n",
    "\n",
    "# ===== TabPFN stacking feature (optional) =====\n",
    "# 事前学習済みのTabPFNからOOF予測を作り、1次元の補助特徴量として追加\n",
    "STACK_TABPFN = True  # Falseで無効化\n",
    "TABPFN_N_SPLITS = 13\n",
    "TABPFN_ENSEMBLE = 16\n",
    "TABPFN_BINS = 30  # Regressorが無い環境では分類のビニングで近似\n",
    "\n",
    "def add_tabpfn_stack_feature(X_df, y_series, test_df, n_splits=TABPFN_N_SPLITS):\n",
    "    import os\n",
    "    # Allow TabPFN on CPU for large datasets\n",
    "    if not torch.cuda.is_available():\n",
    "        os.environ['TABPFN_ALLOW_CPU_LARGE_DATASET'] = '1'\n",
    "    try:\n",
    "        from tabpfn import TabPFNRegressor, TabPFNClassifier\n",
    "    except Exception as e:\n",
    "        # 必要ならインストールを試行（失敗したらスキップ）\n",
    "        try:\n",
    "            import sys, subprocess\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'tabpfn'])\n",
    "            from tabpfn import TabPFNRegressor, TabPFNClassifier\n",
    "        except Exception as ie:\n",
    "            print('TabPFN not available; using pure NumPy ridge fallback. Error:', ie)\n",
    "            # ===== Pure NumPy fallback (no scikit-learn dependency) =====\n",
    "            rng = np.random.RandomState(SEED)\n",
    "            n = len(X_df)\n",
    "            idx = np.arange(n)\n",
    "            rng.shuffle(idx)\n",
    "            fold_sizes = (n // n_splits) * np.ones(n_splits, dtype=int)\n",
    "            fold_sizes[: n % n_splits] += 1\n",
    "            current = 0\n",
    "            folds = []\n",
    "            for fs in fold_sizes:\n",
    "                folds.append(idx[current: current + fs])\n",
    "                current += fs\n",
    "            def _impute_mean(A, ref=None):\n",
    "                B = np.array(A, dtype=float, copy=True)\n",
    "                if ref is None:\n",
    "                    ref = B\n",
    "                col_mean = np.nanmean(ref, axis=0)\n",
    "                inds = np.where(np.isnan(B))\n",
    "                B[inds] = np.take(col_mean, inds[1])\n",
    "                return B\n",
    "            def _standardize(A, ref):\n",
    "                mu = np.mean(ref, axis=0)\n",
    "                sd = np.std(ref, axis=0) + 1e-8\n",
    "                return (A - mu) / sd, mu, sd\n",
    "            def _ridge_fit(Xm, ym, lam=1e-1):\n",
    "                XTX = Xm.T @ Xm\n",
    "                nfeat = XTX.shape[0]\n",
    "                A = XTX + lam * np.eye(nfeat)\n",
    "                b = Xm.T @ ym\n",
    "                return np.linalg.solve(A, b)\n",
    "            X_np = X_df.values\n",
    "            test_np = test_df.values\n",
    "            y_np = y_series.values if hasattr(y_series, 'values') else np.asarray(y_series)\n",
    "            oof = np.zeros(n, dtype=float)\n",
    "            test_pred_folds = []\n",
    "            for i in range(n_splits):\n",
    "                va_idx = folds[i]\n",
    "                tr_idx = np.concatenate([folds[j] for j in range(n_splits) if j != i])\n",
    "                X_tr = X_np[tr_idx]; X_va = X_np[va_idx]; y_tr = y_np[tr_idx]\n",
    "                X_tr_imp = _impute_mean(X_tr)\n",
    "                X_va_imp = _impute_mean(X_va, ref=X_tr_imp)\n",
    "                test_imp_local = _impute_mean(test_np, ref=X_tr_imp)\n",
    "                X_tr_std, mu, sd = _standardize(X_tr_imp, X_tr_imp)\n",
    "                X_va_std = (X_va_imp - mu) / sd\n",
    "                test_std = (test_imp_local - mu) / sd\n",
    "                w = _ridge_fit(X_tr_std, y_tr, lam=1e-1)\n",
    "                oof[va_idx] = X_va_std @ w\n",
    "                test_pred_folds.append(test_std @ w)\n",
    "            test_pred_mean = np.mean(np.column_stack(test_pred_folds), axis=1)\n",
    "            X_new = X_df.copy(); test_new = test_df.copy()\n",
    "            X_new['tabpfn_pred'] = oof\n",
    "            test_new['tabpfn_pred'] = test_pred_mean\n",
    "            return X_new, test_new, oof\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    oof = np.zeros(len(X_df), dtype=float)\n",
    "    test_pred_folds = []\n",
    "    X_np = X_df.values\n",
    "    test_np = test_df.values\n",
    "    y_np = y_series.values if hasattr(y_series, 'values') else np.asarray(y_series)\n",
    "    for tr_idx, va_idx in kf.split(X_np):\n",
    "        X_tr = X_np[tr_idx]\n",
    "        X_va = X_np[va_idx]\n",
    "        y_tr = y_np[tr_idx]\n",
    "        # 欠損は学習FoldでImputerをfit（リーク防止）\n",
    "        imp = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
    "        X_tr_imp = imp.fit_transform(X_tr)\n",
    "        X_va_imp = imp.transform(X_va)\n",
    "        test_imp_local = imp.transform(test_np)\n",
    "        try:\n",
    "            # 回帰器が利用可能な場合（バージョン差異を吸収）\n",
    "            try:\n",
    "                reg = TabPFNRegressor(seed=SEED, device=device)\n",
    "            except TypeError:\n",
    "                reg = TabPFNRegressor()\n",
    "            try:\n",
    "                reg.fit(X_tr_imp, y_tr, ignore_pretraining_limits=True)\n",
    "            except TypeError:\n",
    "                reg.fit(X_tr_imp, y_tr)\n",
    "            try:\n",
    "                oof[va_idx] = reg.predict(X_va_imp, N_ensemble_configurations=TABPFN_ENSEMBLE).astype(float)\n",
    "                test_pred = reg.predict(test_imp_local, N_ensemble_configurations=TABPFN_ENSEMBLE).astype(float)\n",
    "            except TypeError:\n",
    "                oof[va_idx] = reg.predict(X_va_imp).astype(float)\n",
    "                test_pred = reg.predict(test_imp_local).astype(float)\n",
    "        except Exception as e:\n",
    "            # 分類の確率出力で回帰を近似（分位ビンの期待値）\n",
    "            bins = np.quantile(y_tr, np.linspace(0.0, 1.0, TABPFN_BINS + 1))\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf\n",
    "            y_tr_binned = np.digitize(y_tr, bins[1:-1], right=True)\n",
    "            try:\n",
    "                clf = TabPFNClassifier(seed=SEED, device=device)\n",
    "            except TypeError:\n",
    "                clf = TabPFNClassifier()\n",
    "            try:\n",
    "                clf.fit(X_tr_imp, y_tr_binned, ignore_pretraining_limits=True)\n",
    "            except TypeError:\n",
    "                clf.fit(X_tr_imp, y_tr_binned)\n",
    "            try:\n",
    "                proba_va = clf.predict_proba(X_va_imp, N_ensemble_configurations=TABPFN_ENSEMBLE)\n",
    "                proba_te = clf.predict_proba(test_imp_local, N_ensemble_configurations=TABPFN_ENSEMBLE)\n",
    "            except TypeError:\n",
    "                proba_va = clf.predict_proba(X_va_imp)\n",
    "                proba_te = clf.predict_proba(test_imp_local)\n",
    "            mids_all = (bins[:-1] + bins[1:]) / 2.0\n",
    "            try:\n",
    "                classes = clf.classes_.astype(int)\n",
    "                mids_used = mids_all[classes]\n",
    "            except Exception:\n",
    "                mids_used = mids_all\n",
    "            oof[va_idx] = (proba_va * mids_used).sum(axis=1)\n",
    "            test_pred = (proba_te * mids_used).sum(axis=1)\n",
    "        test_pred_folds.append(test_pred)\n",
    "    test_pred_mean = np.mean(np.column_stack(test_pred_folds), axis=1)\n",
    "    X_new = X_df.copy()\n",
    "    test_new = test_df.copy()\n",
    "    X_new['tabpfn_pred'] = oof\n",
    "    test_new['tabpfn_pred'] = test_pred_mean\n",
    "    return X_new, test_new, oof\n",
    "\n",
    "# 学習/検証分割の前にTabPFN特徴を追加\n",
    "if 'STACK_TABPFN' not in globals():\n",
    "    STACK_TABPFN = True\n",
    "if STACK_TABPFN:\n",
    "    try:\n",
    "        X, test, _ = add_tabpfn_stack_feature(X, y, test)\n",
    "        if isinstance(X, pd.DataFrame) and 'tabpfn_pred' in X.columns:\n",
    "            print('Stacked feature added: tabpfn_pred')\n",
    "        else:\n",
    "            print('Stacking skipped: no feature added')\n",
    "    except Exception as e:\n",
    "        print('Stacking failed; continuing without it. Error:', e)\n",
    "\n",
    "def make_feature_transformer(name):\n",
    "    key = (name or 'none').lower()\n",
    "    if key == 'standard':\n",
    "        return StandardScaler()\n",
    "    if key == 'robust':\n",
    "        return RobustScaler()\n",
    "    if key == 'quantile':\n",
    "        return QuantileTransformer(output_distribution=QUANTILE_OUTPUT, random_state=SEED)\n",
    "    if key == 'power':\n",
    "        return PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    return None\n",
    "\n",
    "def fit_target_transform(y, name):\n",
    "    key = (name or 'none').lower()\n",
    "    yy = pd.Series(y).astype(float)\n",
    "    if key == 'log1p':\n",
    "        return np.log1p(yy).astype(np.float32).values, {'name':'log1p'}\n",
    "    if key == 'standard':\n",
    "        mu = float(yy.mean()); sd = float(yy.std() + 1e-8)\n",
    "        return ((yy - mu)/sd).astype(np.float32).values, {'name':'standard','mean':mu,'std':sd}\n",
    "    return yy.astype(np.float32).values, {'name':'none'}\n",
    "\n",
    "def apply_target_transform(y, params):\n",
    "    key = (params.get('name') or 'none').lower()\n",
    "    yy = pd.Series(y).astype(float)\n",
    "    if key == 'log1p':\n",
    "        return np.log1p(yy).astype(np.float32).values\n",
    "    if key == 'standard':\n",
    "        mu = params['mean']; sd = params['std']\n",
    "        return ((yy - mu)/sd).astype(np.float32).values\n",
    "    return yy.astype(np.float32).values\n",
    "\n",
    "def inverse_target_transform(arr, params):\n",
    "    key = (params.get('name') or 'none').lower()\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    if key == 'log1p':\n",
    "        return np.expm1(a)\n",
    "    if key == 'standard':\n",
    "        return a * params['std'] + params['mean']\n",
    "    return a\n",
    "\n",
    "# Split first to avoid leakage\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Impute missing values (fit on train only)\n",
    "imputer = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
    "X_train_imp = imputer.fit_transform(X_train_raw)\n",
    "X_val_imp = imputer.transform(X_val_raw)\n",
    "test_imp = imputer.transform(test)\n",
    "\n",
    "# Feature transform (fit on train only)\n",
    "_ft = make_feature_transformer(FEATURE_TRANSFORM)\n",
    "if _ft is None:\n",
    "    X_train = X_train_imp\n",
    "    X_val = X_val_imp\n",
    "    test_scaled = test_imp\n",
    "else:\n",
    "    X_train = _ft.fit_transform(X_train_imp)\n",
    "    X_val = _ft.transform(X_val_imp)\n",
    "    test_scaled = _ft.transform(test_imp)\n",
    "\n",
    "# Target transform (fit on train only)\n",
    "y_train_t, TARGET_PARAMS_SINGLE = fit_target_transform(y_train, TARGET_TRANSFORM)\n",
    "y_val_t = apply_target_transform(y_val, TARGET_PARAMS_SINGLE)\n",
    "y_train_proc = pd.Series(y_train_t, index=y_train.index)\n",
    "y_val_proc = pd.Series(y_val_t, index=y_val.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:36:26.006391Z",
     "iopub.status.busy": "2025-11-16T13:36:26.005447Z",
     "iopub.status.idle": "2025-11-16T13:36:26.013325Z",
     "shell.execute_reply": "2025-11-16T13:36:26.012345Z",
     "shell.execute_reply.started": "2025-11-16T13:36:26.006358Z"
    },
    "id": "TaFE3dtqj6fT"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OceanChemistryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        # y は pandas.Series でも numpy.ndarray でも受け付ける\n",
    "        y_arr = y.values if hasattr(y, 'values') else y\n",
    "        self.y = torch.tensor(y_arr, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = OceanChemistryDataset(X_train, y_train_proc)\n",
    "val_dataset = OceanChemistryDataset(X_val, y_val_proc)\n",
    "\n",
    "# === 調整ポイント: 学習のバッチ設定 ===\n",
    "# ・batch_size: 32/64/128/256 あたりで比較（大きいほど安定・速いが汎化は要CVで検証）→実際１が最強\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:36:29.499859Z",
     "iopub.status.busy": "2025-11-16T13:36:29.499037Z",
     "iopub.status.idle": "2025-11-16T13:36:29.508403Z",
     "shell.execute_reply": "2025-11-16T13:36:29.507563Z",
     "shell.execute_reply.started": "2025-11-16T13:36:29.499829Z"
    },
    "id": "8kUw3FU7j6fT",
    "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1163, 16)\n"
     ]
    }
   ],
   "source": [
    "# ===== MLPハイパラ（ここを主に調整） =====\n",
    "# DROPOUT_P: 0.1〜0.3 推奨（0で無効）\n",
    "DROPOUT_P = 0  # Dropout probability (0.1-0.3 recommended)\n",
    "# ACTIVATION_NAME: 'ReLU'|'GELU'|'SiLU'|'Tanh'|'LeakyReLU'\n",
    "ACTIVATION_NAME = 'SiLU'  # Options: ReLU, GELU, SiLU, Tanh, LeakyReLU,mish\n",
    "\n",
    "def make_activation(name):\n",
    "    try:\n",
    "        key = (name or 'ReLU').lower()\n",
    "    except Exception:\n",
    "        key = 'relu'\n",
    "    if key == 'relu':\n",
    "        return nn.ReLU()\n",
    "    if key in ('mish','mesh'):\n",
    "        # Mish activation (fallback to SiLU if not available)\",\n",
    "        try:\n",
    "            return nn.Mish()\n",
    "        except AttributeError:\n",
    "            return nn.SiLU()\n",
    "    if key == 'gelu':\n",
    "        return nn.GELU()\n",
    "    if key in ('silu','swish'):\n",
    "        return nn.SiLU()\n",
    "    if key == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    if key in ('leakyrelu','lrelu'):\n",
    "        return nn.LeakyReLU(0.01)\n",
    "    return nn.ReLU()\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME):\n",
    "\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        # 隠れ層ユニット数（今は1024。64〜1024で比較してみてください）\n",
    "        self.fc1 = nn.Linear(input_size, 1500)\n",
    "\n",
    "        self.act1 = make_activation(activation_name)\n",
    "\n",
    "        self.drop1 = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc3 = nn.Linear(1500, 1)  # Output layer for regression\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = MLPModel(input_size=X_train.shape[1], dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME)\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T13:54:18.614585Z",
     "iopub.status.busy": "2025-11-16T13:54:18.614207Z",
     "iopub.status.idle": "2025-11-16T14:06:20.042727Z",
     "shell.execute_reply": "2025-11-16T14:06:20.041734Z",
     "shell.execute_reply.started": "2025-11-16T13:54:18.614563Z"
    },
    "id": "TZrodbLrj6fT",
    "outputId": "15a041e0-6c82-455e-c55a-55bd117814ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, LR: 1.00e-03, Train Loss: 2.2425809895566897, Validation Loss: 2.369087106711471\n",
      "Epoch 101/5000, LR: 5.00e-04, Train Loss: 1.7059155793368868, Validation Loss: 2.2134758945029627\n",
      "Epoch 201/5000, LR: 1.25e-04, Train Loss: 1.423680633152751, Validation Loss: 2.1572942180987895\n",
      "Epoch 301/5000, LR: 3.13e-05, Train Loss: 1.358600359458095, Validation Loss: 2.144392602222482\n",
      "Epoch 401/5000, LR: 7.81e-06, Train Loss: 1.3402314905528867, Validation Loss: 2.1402200941265246\n",
      "Epoch 501/5000, LR: 1.95e-06, Train Loss: 1.3350113428430832, Validation Loss: 2.141985722372622\n",
      "Early stopping at epoch 507; best val: 2.041853\n"
     ]
    }
   ],
   "source": [
    "WEIGHT_DECAY = 1e-4  # 1e-4 to 1e-3 recommended\n",
    "EARLY_STOPPING_PATIENCE = 500  # epochs with no improvement before stop\n",
    "EARLY_STOPPING_MIN_DELTA = 1e-4  # minimum improvement to reset patience\n",
    "\n",
    "# Model selection toggle: 'BP' (PyTorch backprop), 'ELM', 'RBF'\n",
    "MODEL_NAME = 'BP'\n",
    "\n",
    "# ELM hyperparameters\n",
    "ELM_HIDDEN = 512\n",
    "ELM_REG = 1e-2  # ridge regularization\n",
    "ELM_ACTIVATION = 'relu'  # relu|tanh|sigmoid\n",
    "\n",
    "# RBF hyperparameters\n",
    "RBF_UNITS = 100\n",
    "RBF_REG = 1e-2\n",
    "RBF_SIGMA_SCALE = 1.0  # scale factor for sigma derived from centers\n",
    "\n",
    "import torch.optim as optim\n",
    "# ===== 最適化/損失/早期終了（主な調整ポイント） =====\n",
    "# OPTIMIZER_NAME: 'Adam'|'AdamW'|'SGD'|'RMSprop'|'Adagrad'\n",
    "#   ・SGDを使うなら OPTIMIZER_PARAMS={'momentum':0.9,'nesterov':True} など\n",
    "# WEIGHT_DECAY: 1e-4〜1e-3 推奨（L2正則化。大きすぎると学習が弱まる）\n",
    "# LOSS_NAME: 'SmoothL1'|'MSE'|'L1'|'Huber'（Huberはdelta、SmoothL1はbetaをLOSS_PARAMSで指定可）\n",
    "# EARLY_STOPPING_PATIENCE/MIN_DELTA: 早期終了の判定\n",
    "# 学習率lrはmake_optimizerの引数で指定。ReduceLROnPlateauで自動減衰\n",
    "\n",
    "# Optimizer toggle\n",
    "OPTIMIZER_NAME = 'AdamW'  # Options: 'Adam','AdamW','SGD','RMSprop','Adagrad'\n",
    "OPTIMIZER_PARAMS = {}  # e.g., {'momentum':0.9} for SGD\n",
    "\n",
    "def make_optimizer(name, params, **kwargs):\n",
    "    try:\n",
    "        key = (name or 'Adam').lower()\n",
    "    except Exception:\n",
    "        key = 'adam'\n",
    "    lr = kwargs.get('lr', 1e-3)\n",
    "    wd = kwargs.get('weight_decay', 0.0)\n",
    "    if key == 'adamw':\n",
    "        return optim.AdamW(params, lr=lr, weight_decay=wd)\n",
    "    if key == 'sgd':\n",
    "        return optim.SGD(params, lr=lr, momentum=kwargs.get('momentum', 0.9), nesterov=kwargs.get('nesterov', False), weight_decay=wd)\n",
    "    if key == 'rmsprop':\n",
    "        return optim.RMSprop(params, lr=lr, momentum=kwargs.get('momentum', 0.0), alpha=kwargs.get('alpha', 0.99), weight_decay=wd)\n",
    "    if key == 'adagrad':\n",
    "        return optim.Adagrad(params, lr=lr, weight_decay=wd)\n",
    "    return optim.Adam(params, lr=lr, weight_decay=wd)\n",
    "\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "\n",
    "LOSS_NAME = 'SmoothL1'  # Options: 'SmoothL1', 'MSE', 'L1', 'Huber'\n",
    "LOSS_PARAMS = {}  # e.g., {'beta': 1.0} for SmoothL1 or {'delta': 1.0} for Huber\n",
    "\n",
    "def make_loss(name, **kwargs):\n",
    "    try:\n",
    "        key = (name or 'SmoothL1').lower()\n",
    "    except Exception:\n",
    "        key = 'smoothl1'\n",
    "    if key in ('mse','mseloss'):\n",
    "        return nn.MSELoss()\n",
    "    if key in ('l1','mae','l1loss'):\n",
    "        return nn.L1Loss()\n",
    "    if key in ('huber','huberloss'):\n",
    "        delta = kwargs.get('delta', 1.0)\n",
    "        try:\n",
    "            return nn.HuberLoss(delta=delta)\n",
    "        except TypeError:\n",
    "            return nn.SmoothL1Loss()\n",
    "    # Default SmoothL1\n",
    "    beta = kwargs.get('beta', 1.0)\n",
    "    try:\n",
    "        return nn.SmoothL1Loss(beta=beta)\n",
    "    except TypeError:\n",
    "        return nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "criterion = make_loss(LOSS_NAME, **LOSS_PARAMS)\n",
    "\n",
    "optimizer = make_optimizer(OPTIMIZER_NAME, model.parameters(), lr=0.001, weight_decay=WEIGHT_DECAY, **OPTIMIZER_PARAMS)\n",
    "\n",
    "# LR scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6)\n",
    "\n",
    "\n",
    "def _np_activation(name, X):\n",
    "    key = (name or 'relu').lower()\n",
    "    if key == 'relu':\n",
    "        return (X > 0) * X\n",
    "    if key == 'tanh':\n",
    "        return np.tanh(X)\n",
    "    if key in ('sigmoid','logistic'):\n",
    "        return 1.0 / (1.0 + np.exp(-X))\n",
    "    return (X > 0) * X\n",
    "\n",
    "def _ridge_solve(H, y, reg):\n",
    "    # Solve (H^T H + reg I) w = H^T y\n",
    "    HtH = H.T @ H\n",
    "    n = HtH.shape[0]\n",
    "    A = HtH + reg * np.eye(n)\n",
    "    b = H.T @ y\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "def fit_elm(X, y, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=42):\n",
    "    rs = np.random.RandomState(seed)\n",
    "    W = rs.normal(scale=1.0, size=(X.shape[1], hidden))\n",
    "    b = rs.normal(scale=1.0, size=(hidden,))\n",
    "    H = _np_activation(act, X @ W + b)\n",
    "    beta = _ridge_solve(H, y.astype(float), reg)\n",
    "    return {'W': W, 'b': b, 'beta': beta, 'act': act}\n",
    "\n",
    "def predict_elm(model_dict, X):\n",
    "    W = model_dict['W']; b = model_dict['b']; act = model_dict['act']; beta = model_dict['beta']\n",
    "    H = _np_activation(act, X @ W + b)\n",
    "    return H @ beta\n",
    "\n",
    "def _rbf_design(X, centers, gamma):\n",
    "    # Compute squared Euclidean distances efficiently\n",
    "    X2 = np.sum(X*X, axis=1, keepdims=True)\n",
    "    C2 = np.sum(centers*centers, axis=1)[None, :]\n",
    "    dist2 = X2 + C2 - 2.0 * (X @ centers.T)\n",
    "    return np.exp(-gamma * dist2)\n",
    "\n",
    "def fit_rbf(X, y, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=42):\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=units, random_state=seed, n_init=10)\n",
    "    centers = km.fit(X).cluster_centers_\n",
    "    # Estimate sigma from center distances\n",
    "    from scipy.spatial.distance import cdist\n",
    "    try:\n",
    "        import numpy as _np\n",
    "        pair = _np.linalg.norm(centers[:,None,:]-centers[None,:,:], axis=2)\n",
    "    except Exception:\n",
    "        pair = np.zeros((units, units))\n",
    "    # Use median of nearest-neighbor distances\n",
    "    nn = []\n",
    "    for i in range(units):\n",
    "        vals = np.sort(pair[i][pair[i]>0])\n",
    "        if vals.size>0:\n",
    "            nn.append(vals[0])\n",
    "    sigma = (np.median(nn) if len(nn)>0 else 1.0) * sigma_scale\n",
    "    sigma = max(sigma, 1e-6)\n",
    "    gamma = 1.0/(2.0*sigma*sigma)\n",
    "    Phi = _rbf_design(X, centers, gamma)\n",
    "    w = _ridge_solve(Phi, y.astype(float), reg)\n",
    "    return {'centers': centers, 'gamma': gamma, 'w': w}\n",
    "\n",
    "def predict_rbf(model_dict, X):\n",
    "    centers = model_dict['centers']; gamma = model_dict['gamma']; w = model_dict['w']\n",
    "    Phi = _rbf_design(X, centers, gamma)\n",
    "    return Phi @ w\n",
    "\n",
    "\n",
    "# Training function\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5000, scheduler=None):\n",
    "\n",
    "    import copy\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for X_batch, y_batch in val_loader:\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), y_batch)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss_avg = val_loss/len(val_loader) if len(val_loader)>0 else val_loss\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss_avg)\n",
    "\n",
    "        # Early stopping (only if we have validation batches)\n",
    "        if len(val_loader) > 0:\n",
    "            if best_val - val_loss_avg > EARLY_STOPPING_MIN_DELTA:\n",
    "                best_val = val_loss_avg\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch+1}; best val: {best_val:.6f}\")\n",
    "                if best_state is not None:\n",
    "                    model.load_state_dict(best_state)\n",
    "                break\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, LR: {current_lr:.2e}, Train Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss_avg}\")\n",
    "\n",
    "    # Load best state at the end if available\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "# Train based on selected model\n",
    "\n",
    "if MODEL_NAME.upper() == 'BP':\n",
    "    train_model(model, train_loader, val_loader, epochs=5000, scheduler=scheduler)\n",
    "elif MODEL_NAME.upper() == 'ELM':\n",
    "    print('Training ELM...')\n",
    "    ELM_MODEL = fit_elm(X_train, y_train, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=SEED)\n",
    "elif MODEL_NAME.upper() == 'RBF':\n",
    "    print('Training RBF...')\n",
    "    RBF_MODEL = fit_rbf(X_train, y_train, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=SEED)\n",
    "else:\n",
    "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== KFold / OOF with per-fold preprocessing and test ensembling =====\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_pred = np.zeros(len(X), dtype=np.float32)\n",
    "test_pred = np.zeros((N_SPLITS, len(test)), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Fold {fold}/{N_SPLITS}\")\n",
    "\n",
    "    # Split raw features and target\n",
    "    X_tr_raw = X.iloc[tr_idx].copy()\n",
    "    X_val_raw = X.iloc[val_idx].copy()\n",
    "    y_tr = y.iloc[tr_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "\n",
    "    # Imputer (fit on fold-train only)\n",
    "    imputer = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
    "    X_tr_imp = imputer.fit_transform(X_tr_raw)\n",
    "    X_val_imp = imputer.transform(X_val_raw)\n",
    "    test_imp = imputer.transform(test)\n",
    "\n",
    "    # Feature transform (fit on fold-train only)\n",
    "    ft = make_feature_transformer(FEATURE_TRANSFORM)\n",
    "    if ft is None:\n",
    "        X_tr_proc = X_tr_imp\n",
    "        X_val_proc = X_val_imp\n",
    "        test_proc = test_imp\n",
    "    else:\n",
    "        X_tr_proc = ft.fit_transform(X_tr_imp)\n",
    "        X_val_proc = ft.transform(X_val_imp)\n",
    "        test_proc = ft.transform(test_imp)\n",
    "\n",
    "    # Target transform (fit on fold-train only)\n",
    "    y_tr_t, target_params = fit_target_transform(y_tr, TARGET_TRANSFORM)\n",
    "    y_val_t = apply_target_transform(y_val, target_params)\n",
    "    y_tr_proc = pd.Series(y_tr_t, index=y_tr.index)\n",
    "    y_val_proc = pd.Series(y_val_t, index=y_val.index)\n",
    "\n",
    "    # Datasets and loaders\n",
    "    train_ds = OceanChemistryDataset(X_tr_proc, y_tr_proc)\n",
    "    val_ds = OceanChemistryDataset(X_val_proc, y_val_proc)\n",
    "    train_loader_cv = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "    val_loader_cv = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    # New model / optimizer / loss per fold\n",
    "    fold_model = MLPModel(input_size=X_tr_proc.shape[1],\n",
    "                          dropout_p=DROPOUT_P,\n",
    "                          activation_name=ACTIVATION_NAME)\n",
    "\n",
    "    # Use global names so train_model can see them\n",
    "    criterion = make_loss(LOSS_NAME, **LOSS_PARAMS)\n",
    "    optimizer = make_optimizer(OPTIMIZER_NAME, fold_model.parameters(),\n",
    "                               lr=0.001, weight_decay=WEIGHT_DECAY, **OPTIMIZER_PARAMS)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # Train for this fold\n",
    "    train_model(fold_model, train_loader_cv, val_loader_cv,\n",
    "                epochs=5000, scheduler=scheduler)\n",
    "\n",
    "    # OOF prediction (back to original target scale)\n",
    "    fold_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_tensor = torch.tensor(X_val_proc, dtype=torch.float32)\n",
    "        pred_val_t = fold_model(val_tensor).squeeze().cpu().numpy()\n",
    "    pred_val = inverse_target_transform(pred_val_t, target_params)\n",
    "    oof_pred[val_idx] = pred_val\n",
    "\n",
    "    # Test prediction for this fold (back to original scale)\n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.tensor(test_proc, dtype=torch.float32)\n",
    "        pred_test_t = fold_model(test_tensor).squeeze().cpu().numpy()\n",
    "    pred_test = inverse_target_transform(pred_test_t, target_params)\n",
    "    test_pred[fold - 1, :] = pred_test\n",
    "\n",
    "# OOF score on original scale\n",
    "oof_rmse = mean_squared_error(y.values, oof_pred, squared=False)\n",
    "print(f\"KFold OOF RMSE (full train): {oof_rmse:.4f}\")\n",
    "\n",
    "# Mean-ensemble test prediction over folds\n",
    "test_pred_mean = test_pred.mean(axis=0)\n",
    "\n",
    "# Submission using sample_submission id\n",
    "sub_kfold = sub.copy()\n",
    "sub_kfold[\"DIC\"] = test_pred_mean\n",
    "sub_kfold.to_csv(\"submission_kfold.csv\", index=False)\n",
    "print(\"Saved submission_kfold.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T14:06:45.053996Z",
     "iopub.status.busy": "2025-11-16T14:06:45.053173Z",
     "iopub.status.idle": "2025-11-16T14:06:45.063704Z",
     "shell.execute_reply": "2025-11-16T14:06:45.063053Z",
     "shell.execute_reply.started": "2025-11-16T14:06:45.053962Z"
    },
    "id": "CD3YH6Drj6fT"
   },
   "outputs": [],
   "source": [
    "# Predict according to MODEL_NAME\n",
    "if MODEL_NAME.upper() == 'BP':\n",
    "    # Convert the test set into a torch tensor\n",
    "    test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_tensor).squeeze().numpy()\n",
    "elif MODEL_NAME.upper() == 'ELM':\n",
    "    predictions = predict_elm(ELM_MODEL, test_scaled)\n",
    "elif MODEL_NAME.upper() == 'RBF':\n",
    "    predictions = predict_rbf(RBF_MODEL, test_scaled)\n",
    "else:\n",
    "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n",
    "\n",
    "# Inverse target transform if applied (single split)\n",
    "try:\n",
    "    predictions = inverse_target_transform(predictions, TARGET_PARAMS_SINGLE)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\"id\": range(1455, 1455 + len(predictions)), \"DIC\": predictions})\n",
    "submission.to_csv(\"submission35.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7941427,
     "sourceId": 49552,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
