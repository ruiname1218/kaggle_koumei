{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:56:02.791473Z",
     "iopub.status.busy": "2025-11-18T00:56:02.790582Z",
     "iopub.status.idle": "2025-11-18T00:56:02.798257Z",
     "shell.execute_reply": "2025-11-18T00:56:02.797442Z",
     "shell.execute_reply.started": "2025-11-18T00:56:02.791435Z"
    },
    "id": "seed_cell",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 8000\n",
    "import os, random, numpy as np\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# PyTorch seeding (TensorFlow is skipped to avoid environment issues)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:56:04.574777Z",
     "iopub.status.busy": "2025-11-18T00:56:04.574237Z",
     "iopub.status.idle": "2025-11-18T00:56:04.578822Z",
     "shell.execute_reply": "2025-11-18T00:56:04.578114Z",
     "shell.execute_reply.started": "2025-11-18T00:56:04.574755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-18T00:56:06.934683Z",
     "iopub.status.busy": "2025-11-18T00:56:06.934117Z",
     "iopub.status.idle": "2025-11-18T00:57:45.027410Z",
     "shell.execute_reply": "2025-11-18T00:57:45.026538Z",
     "shell.execute_reply.started": "2025-11-18T00:56:06.934659Z"
    },
    "id": "w0miHB_4j6fO",
    "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools, pip\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn<1.7,>=1.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7,>=1.2) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn<1.7,>=1.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn<1.7,>=1.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn<1.7,>=1.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn<1.7,>=1.2) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn<1.7,>=1.2) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tabpfn\n",
      "  Downloading tabpfn-6.0.6-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch<3,>=2.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy<3,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn<1.8,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.12.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (4.15.0)\n",
      "Requirement already satisfied: scipy<2,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.15.3)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.2.3)\n",
      "Requirement already satisfied: einops<0.9,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub<2,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (0.36.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.12.4)\n",
      "Requirement already satisfied: pydantic-settings>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (2.11.0)\n",
      "Collecting eval-type-backport>=0.2.2 (from tabpfn)\n",
      "  Downloading eval_type_backport-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn) (1.5.2)\n",
      "Collecting tabpfn-common-utils>=0.2.7 (from tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn)\n",
      "  Downloading tabpfn_common_utils-0.2.9-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting kditransform>=1.2 (from tabpfn)\n",
      "  Downloading kditransform-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2,>=0.19.0->tabpfn) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.21.6->tabpfn) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->tabpfn) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.8,>=1.2.0->tabpfn) (3.6.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.1->tabpfn)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.1->tabpfn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.1->tabpfn) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.48 in /usr/local/lib/python3.11/dist-packages (from kditransform>=1.2->tabpfn) (0.60.0)\n",
      "Collecting scikit-learn<1.8,>=1.2.0 (from tabpfn)\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.48->kditransform>=1.2->tabpfn) (0.43.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->tabpfn) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings>=2.10.1->tabpfn) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->tabpfn) (1.17.0)\n",
      "Requirement already satisfied: platformdirs>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (4.5.0)\n",
      "Collecting posthog~=6.7 (from tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn)\n",
      "  Downloading posthog-6.9.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog~=6.7->tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog~=6.7->tabpfn-common-utils>=0.2.7->tabpfn-common-utils[telemetry-interactive]>=0.2.7->tabpfn) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<2,>=0.19.0->tabpfn) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<2,>=0.19.0->tabpfn) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<2,>=0.19.0->tabpfn) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<2,>=0.19.0->tabpfn) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.1->tabpfn) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21.6->tabpfn) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21.6->tabpfn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.21.6->tabpfn) (2022.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.21.6->tabpfn) (2024.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.21.6->tabpfn) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.21.6->tabpfn) (2024.2.0)\n",
      "Downloading tabpfn-6.0.6-py3-none-any.whl (551 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.9/551.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading eval_type_backport-0.3.0-py3-none-any.whl (6.1 kB)\n",
      "Downloading kditransform-1.2.0-py3-none-any.whl (20 kB)\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tabpfn_common_utils-0.2.9-py3-none-any.whl (34 kB)\n",
      "Downloading posthog-6.9.3-py3-none-any.whl (144 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, eval-type-backport, backoff, posthog, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, scikit-learn, tabpfn-common-utils, kditransform, tabpfn\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82/17\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━\u001b[0m \u001b[32m 0/17\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.820m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:0m \u001b[32m 0/17\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82━━━━━━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82m \u001b[32m 2/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82[0m \u001b[32m 2/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82[0m \u001b[32m 3/17\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/17\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 4/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82[0m \u001b[32m 4/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2━━━━━━━━\u001b[0m \u001b[32m 6/17\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [posthog]ublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\u001b[0m \u001b[32m 9/17\u001b[0m [posthog]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [posthog]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3━━━━━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75━━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:0m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75━━━━━━━━━\u001b[0m \u001b[32m11/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83[0m \u001b[32m11/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: scikit-learn\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: scikit-learn 1.2.2━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling scikit-learn-1.2.2:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled scikit-learn-1.2.2[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m13/17\u001b[0m [scikit-learn]u12]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [tabpfn]16/17\u001b[0m [tabpfn]learn]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 eval-type-backport-0.3.0 kditransform-1.2.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 posthog-6.9.3 scikit-learn-1.7.2 tabpfn-6.0.6 tabpfn-common-utils-0.2.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\n",
      "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\n",
      "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pip setuptools wheel\n",
    "%pip install 'scikit-learn>=1.2,<1.7'\n",
    "%pip install -U tabpfn\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:58:26.569693Z",
     "iopub.status.busy": "2025-11-18T00:58:26.568611Z",
     "iopub.status.idle": "2025-11-18T00:58:26.609902Z",
     "shell.execute_reply": "2025-11-18T00:58:26.609312Z",
     "shell.execute_reply.started": "2025-11-18T00:58:26.569645Z"
    },
    "id": "wmcUA-Kyj6fR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:58:29.075027Z",
     "iopub.status.busy": "2025-11-18T00:58:29.074664Z",
     "iopub.status.idle": "2025-11-18T00:59:08.179740Z",
     "shell.execute_reply": "2025-11-18T00:59:08.179015Z",
     "shell.execute_reply.started": "2025-11-18T00:58:29.075008Z"
    },
    "id": "2SYojsJUj6fS",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.36.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.1.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface_hub) (8.3.0)\n",
      "Downloading huggingface_hub-1.1.4-py3-none-any.whl (515 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "tokenizers 0.21.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.1.4 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "transformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-1.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6e6ea171bd4ee993f33ffe9e3e74c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tabpfn-v2.5-regressor-v2.5_default.ckpt:   0%|          | 0.00/40.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b950837f6c452380377315665952eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked feature added: tabpfn_pred\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler, QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pip install -U huggingface_hub\n",
    "from huggingface_hub import login; login(\"\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "# train = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
    "# test = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
    "\n",
    "# Drop unneeded columns and handle missing values\n",
    "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
    "\n",
    "# Find common columns between train and test (excluding \"DIC\" from train)\n",
    "common_columns = train.drop(columns=[\"DIC\"]).columns.intersection(test.columns)\n",
    "\n",
    "# Select the common columns for both train and test\n",
    "X = train[common_columns].copy()\n",
    "y = train[\"DIC\"]\n",
    "test = test[common_columns].copy()\n",
    "\n",
    "\n",
    "# Toggle for feature engineering (set to False to disable)\n",
    "# === 調整ポイント: 特徴量ENGのON/OFF（効果比較に便利） ===\n",
    "# True: 追加特徴（N_tot, 比率, 交互作用, sin/cos 等）を使う\n",
    "# False: 生の共通列のみで学習\n",
    "FE_ENABLED = False\n",
    "\n",
    "if FE_ENABLED:\n",
    "    # Feature engineering (deterministic; same for train/test)\n",
    "    # Totals and ratios (avoid divide-by-zero -> NaN; imputer will handle)\n",
    "    X['N_tot'] = X['NO3uM'] + X['NO2uM'] + X['NH3uM']\n",
    "    test['N_tot'] = test['NO3uM'] + test['NO2uM'] + test['NH3uM']\n",
    "    X['N_to_P'] = np.where(X['PO4uM'] == 0, np.nan, X['NO3uM'] / X['PO4uM'])\n",
    "    test['N_to_P'] = np.where(test['PO4uM'] == 0, np.nan, test['NO3uM'] / test['PO4uM'])\n",
    "    X['Si_to_N'] = np.where(X['NO3uM'] == 0, np.nan, X['SiO3uM'] / X['NO3uM'])\n",
    "    test['Si_to_N'] = np.where(test['NO3uM'] == 0, np.nan, test['SiO3uM'] / test['NO3uM'])\n",
    "    # Interactions\n",
    "    X['Depth_Temp'] = X['R_Depth'] * X['R_TEMP']\n",
    "    test['Depth_Temp'] = test['R_Depth'] * test['R_TEMP']\n",
    "    X['Sal_Temp'] = X['R_Sal'] * X['R_TEMP']\n",
    "    test['Sal_Temp'] = test['R_Sal'] * test['R_TEMP']\n",
    "    # Geographic trig features\n",
    "    X['sin_lat'] = np.sin(np.radians(X['Lat_Dec']))\n",
    "    X['cos_lat'] = np.cos(np.radians(X['Lat_Dec']))\n",
    "    X['sin_lon'] = np.sin(np.radians(X['Lon_Dec']))\n",
    "    X['cos_lon'] = np.cos(np.radians(X['Lon_Dec']))\n",
    "    test['sin_lat'] = np.sin(np.radians(test['Lat_Dec']))\n",
    "    test['cos_lat'] = np.cos(np.radians(test['Lat_Dec']))\n",
    "    test['sin_lon'] = np.sin(np.radians(test['Lon_Dec']))\n",
    "    test['cos_lon'] = np.cos(np.radians(test['Lon_Dec']))\n",
    "    # Replace infs with NaN to be imputed\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# ===== 調整ポイント: 前処理/変換スイッチ =====\n",
    "# FEATURE_IMPUTER: 欠損補完の方法 ('mean' または 'median')\n",
    "# FEATURE_TRANSFORM: 特徴量変換\n",
    "#   - 'power'    : Yeo-Johnson + 標準化（0/負値OK, 初手におすすめ）\n",
    "#   - 'standard' : 標準化のみ（平均0・分散1）\n",
    "#   - 'robust'   : 外れ値に頑健なスケーリング\n",
    "#   - 'quantile' : 分位変換（出力分布は下のQUANTILE_OUTPUTで指定）\n",
    "#   - 'none'     : 変換しない\n",
    "# TARGET_TRANSFORM: 目的変数変換 ('none'|'log1p'|'standard')\n",
    "#   - 'log1p'    : スケール/外れ値を抑制。予測は自動でexpm1逆変換\n",
    "#   - 'standard' : 平均0・分散1に正規化（予測は平均・分散で戻す）\n",
    "# 変更後は前処理→学習→推論の順に実行してください\n",
    "# Switches for feature/target transforms\n",
    "FEATURE_IMPUTER = 'mean'  # 'mean' or 'median'\n",
    "FEATURE_TRANSFORM = 'power'  # 'power'|'standard'|'robust'|'quantile'|'none'\n",
    "QUANTILE_OUTPUT = 'normal'  # 'normal' or 'uniform'\n",
    "\n",
    "TARGET_TRANSFORM = 'none'  # 'none'|'log1p'|'standard'\n",
    "\n",
    "# ===== TabPFN stacking feature (optional) =====\n",
    "# 事前学習済みのTabPFNからOOF予測を作り、1次元の補助特徴量として追加\n",
    "STACK_TABPFN = True  # Falseで無効化\n",
    "TABPFN_N_SPLITS = 5\n",
    "TABPFN_ENSEMBLE = 16  #計算時間かかるけど。。。上げると精度上がる　　\n",
    "TABPFN_BINS = 30  # Regressorが無い環境では分類のビニングで近似\n",
    "\n",
    "def add_tabpfn_stack_feature(X_df, y_series, test_df, n_splits=TABPFN_N_SPLITS):\n",
    "    import os\n",
    "    # Allow TabPFN on CPU for large datasets\n",
    "    if not torch.cuda.is_available():\n",
    "        os.environ['TABPFN_ALLOW_CPU_LARGE_DATASET'] = '1'\n",
    "    try:\n",
    "        from tabpfn import TabPFNRegressor, TabPFNClassifier\n",
    "    except Exception as e:\n",
    "        # 必要ならインストールを試行（失敗したらスキップ）\n",
    "        try:\n",
    "            import sys, subprocess\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'tabpfn'])\n",
    "            from tabpfn import TabPFNRegressor, TabPFNClassifier\n",
    "        except Exception as ie:\n",
    "            print('TabPFN not available; using pure NumPy ridge fallback. Error:', ie)\n",
    "            # ===== Pure NumPy fallback (no scikit-learn dependency) =====\n",
    "            rng = np.random.RandomState(SEED)\n",
    "            n = len(X_df)\n",
    "            idx = np.arange(n)\n",
    "            rng.shuffle(idx)\n",
    "            fold_sizes = (n // n_splits) * np.ones(n_splits, dtype=int)\n",
    "            fold_sizes[: n % n_splits] += 1\n",
    "            current = 0\n",
    "            folds = []\n",
    "            for fs in fold_sizes:\n",
    "                folds.append(idx[current: current + fs])\n",
    "                current += fs\n",
    "            def _impute_mean(A, ref=None):\n",
    "                B = np.array(A, dtype=float, copy=True)\n",
    "                if ref is None:\n",
    "                    ref = B\n",
    "                col_mean = np.nanmean(ref, axis=0)\n",
    "                inds = np.where(np.isnan(B))\n",
    "                B[inds] = np.take(col_mean, inds[1])\n",
    "                return B\n",
    "            def _standardize(A, ref):\n",
    "                mu = np.mean(ref, axis=0)\n",
    "                sd = np.std(ref, axis=0) + 1e-8\n",
    "                return (A - mu) / sd, mu, sd\n",
    "            def _ridge_fit(Xm, ym, lam=1e-1):\n",
    "                XTX = Xm.T @ Xm\n",
    "                nfeat = XTX.shape[0]\n",
    "                A = XTX + lam * np.eye(nfeat)\n",
    "                b = Xm.T @ ym\n",
    "                return np.linalg.solve(A, b)\n",
    "            X_np = X_df.values\n",
    "            test_np = test_df.values\n",
    "            y_np = y_series.values if hasattr(y_series, 'values') else np.asarray(y_series)\n",
    "            oof = np.zeros(n, dtype=float)\n",
    "            test_pred_folds = []\n",
    "            for i in range(n_splits):\n",
    "                va_idx = folds[i]\n",
    "                tr_idx = np.concatenate([folds[j] for j in range(n_splits) if j != i])\n",
    "                X_tr = X_np[tr_idx]; X_va = X_np[va_idx]; y_tr = y_np[tr_idx]\n",
    "                X_tr_imp = _impute_mean(X_tr)\n",
    "                X_va_imp = _impute_mean(X_va, ref=X_tr_imp)\n",
    "                test_imp_local = _impute_mean(test_np, ref=X_tr_imp)\n",
    "                X_tr_std, mu, sd = _standardize(X_tr_imp, X_tr_imp)\n",
    "                X_va_std = (X_va_imp - mu) / sd\n",
    "                test_std = (test_imp_local - mu) / sd\n",
    "                w = _ridge_fit(X_tr_std, y_tr, lam=1e-1)\n",
    "                oof[va_idx] = X_va_std @ w\n",
    "                test_pred_folds.append(test_std @ w)\n",
    "            test_pred_mean = np.mean(np.column_stack(test_pred_folds), axis=1)\n",
    "            X_new = X_df.copy(); test_new = test_df.copy()\n",
    "            X_new['tabpfn_pred'] = oof\n",
    "            test_new['tabpfn_pred'] = test_pred_mean\n",
    "            return X_new, test_new, oof\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    oof = np.zeros(len(X_df), dtype=float)\n",
    "    test_pred_folds = []\n",
    "    X_np = X_df.values\n",
    "    test_np = test_df.values\n",
    "    y_np = y_series.values if hasattr(y_series, 'values') else np.asarray(y_series)\n",
    "    for tr_idx, va_idx in kf.split(X_np):\n",
    "        X_tr = X_np[tr_idx]\n",
    "        X_va = X_np[va_idx]\n",
    "        y_tr = y_np[tr_idx]\n",
    "        # 欠損は学習FoldでImputerをfit（リーク防止）\n",
    "        imp = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
    "        X_tr_imp = imp.fit_transform(X_tr)\n",
    "        X_va_imp = imp.transform(X_va)\n",
    "        test_imp_local = imp.transform(test_np)\n",
    "        try:\n",
    "            # 回帰器が利用可能な場合（バージョン差異を吸収）\n",
    "            try:\n",
    "                reg = TabPFNRegressor(seed=SEED, device=device)\n",
    "            except TypeError:\n",
    "                reg = TabPFNRegressor()\n",
    "            try:\n",
    "                reg.fit(X_tr_imp, y_tr, ignore_pretraining_limits=True)\n",
    "            except TypeError:\n",
    "                reg.fit(X_tr_imp, y_tr)\n",
    "            try:\n",
    "                oof[va_idx] = reg.predict(X_va_imp, N_ensemble_configurations=TABPFN_ENSEMBLE).astype(float)\n",
    "                test_pred = reg.predict(test_imp_local, N_ensemble_configurations=TABPFN_ENSEMBLE).astype(float)\n",
    "            except TypeError:\n",
    "                oof[va_idx] = reg.predict(X_va_imp).astype(float)\n",
    "                test_pred = reg.predict(test_imp_local).astype(float)\n",
    "        except Exception as e:\n",
    "            # 分類の確率出力で回帰を近似（分位ビンの期待値）\n",
    "            bins = np.quantile(y_tr, np.linspace(0.0, 1.0, TABPFN_BINS + 1))\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf\n",
    "            y_tr_binned = np.digitize(y_tr, bins[1:-1], right=True)\n",
    "            try:\n",
    "                clf = TabPFNClassifier(seed=SEED, device=device)\n",
    "            except TypeError:\n",
    "                clf = TabPFNClassifier()\n",
    "            try:\n",
    "                clf.fit(X_tr_imp, y_tr_binned, ignore_pretraining_limits=True)\n",
    "            except TypeError:\n",
    "                clf.fit(X_tr_imp, y_tr_binned)\n",
    "            try:\n",
    "                proba_va = clf.predict_proba(X_va_imp, N_ensemble_configurations=TABPFN_ENSEMBLE)\n",
    "                proba_te = clf.predict_proba(test_imp_local, N_ensemble_configurations=TABPFN_ENSEMBLE)\n",
    "            except TypeError:\n",
    "                proba_va = clf.predict_proba(X_va_imp)\n",
    "                proba_te = clf.predict_proba(test_imp_local)\n",
    "            mids_all = (bins[:-1] + bins[1:]) / 2.0\n",
    "            try:\n",
    "                classes = clf.classes_.astype(int)\n",
    "                mids_used = mids_all[classes]\n",
    "            except Exception:\n",
    "                mids_used = mids_all\n",
    "            oof[va_idx] = (proba_va * mids_used).sum(axis=1)\n",
    "            test_pred = (proba_te * mids_used).sum(axis=1)\n",
    "        test_pred_folds.append(test_pred)\n",
    "    test_pred_mean = np.mean(np.column_stack(test_pred_folds), axis=1)\n",
    "    X_new = X_df.copy()\n",
    "    test_new = test_df.copy()\n",
    "    X_new['tabpfn_pred'] = oof\n",
    "    test_new['tabpfn_pred'] = test_pred_mean\n",
    "    return X_new, test_new, oof\n",
    "\n",
    "# 学習/検証分割の前にTabPFN特徴を追加\n",
    "if 'STACK_TABPFN' not in globals():\n",
    "    STACK_TABPFN = True\n",
    "if STACK_TABPFN:\n",
    "    try:\n",
    "        X, test, _ = add_tabpfn_stack_feature(X, y, test)\n",
    "        if isinstance(X, pd.DataFrame) and 'tabpfn_pred' in X.columns:\n",
    "            print('Stacked feature added: tabpfn_pred')\n",
    "        else:\n",
    "            print('Stacking skipped: no feature added')\n",
    "    except Exception as e:\n",
    "        print('Stacking failed; continuing without it. Error:', e)\n",
    "\n",
    "def make_feature_transformer(name):\n",
    "    key = (name or 'none').lower()\n",
    "    if key == 'standard':\n",
    "        return StandardScaler()\n",
    "    if key == 'robust':\n",
    "        return RobustScaler()\n",
    "    if key == 'quantile':\n",
    "        return QuantileTransformer(output_distribution=QUANTILE_OUTPUT, random_state=SEED)\n",
    "    if key == 'power':\n",
    "        return PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    return None\n",
    "\n",
    "def fit_target_transform(y, name):\n",
    "    key = (name or 'none').lower()\n",
    "    yy = pd.Series(y).astype(float)\n",
    "    if key == 'log1p':\n",
    "        return np.log1p(yy).astype(np.float32).values, {'name':'log1p'}\n",
    "    if key == 'standard':\n",
    "        mu = float(yy.mean()); sd = float(yy.std() + 1e-8)\n",
    "        return ((yy - mu)/sd).astype(np.float32).values, {'name':'standard','mean':mu,'std':sd}\n",
    "    return yy.astype(np.float32).values, {'name':'none'}\n",
    "\n",
    "def apply_target_transform(y, params):\n",
    "    key = (params.get('name') or 'none').lower()\n",
    "    yy = pd.Series(y).astype(float)\n",
    "    if key == 'log1p':\n",
    "        return np.log1p(yy).astype(np.float32).values\n",
    "    if key == 'standard':\n",
    "        mu = params['mean']; sd = params['std']\n",
    "        return ((yy - mu)/sd).astype(np.float32).values\n",
    "    return yy.astype(np.float32).values\n",
    "\n",
    "def inverse_target_transform(arr, params):\n",
    "    key = (params.get('name') or 'none').lower()\n",
    "    a = np.asarray(arr, dtype=float)\n",
    "    if key == 'log1p':\n",
    "        return np.expm1(a)\n",
    "    if key == 'standard':\n",
    "        return a * params['std'] + params['mean']\n",
    "    return a\n",
    "\n",
    "# Split first to avoid leakage\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Impute missing values (fit on train only)\n",
    "imputer = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
    "X_train_imp = imputer.fit_transform(X_train_raw)\n",
    "X_val_imp = imputer.transform(X_val_raw)\n",
    "test_imp = imputer.transform(test)\n",
    "\n",
    "# Feature transform (fit on train only)\n",
    "_ft = make_feature_transformer(FEATURE_TRANSFORM)\n",
    "if _ft is None:\n",
    "    X_train = X_train_imp\n",
    "    X_val = X_val_imp\n",
    "    test_scaled = test_imp\n",
    "else:\n",
    "    X_train = _ft.fit_transform(X_train_imp)\n",
    "    X_val = _ft.transform(X_val_imp)\n",
    "    test_scaled = _ft.transform(test_imp)\n",
    "\n",
    "# Target transform (fit on train only)\n",
    "y_train_t, TARGET_PARAMS_SINGLE = fit_target_transform(y_train, TARGET_TRANSFORM)\n",
    "y_val_t = apply_target_transform(y_val, TARGET_PARAMS_SINGLE)\n",
    "y_train_proc = pd.Series(y_train_t, index=y_train.index)\n",
    "y_val_proc = pd.Series(y_val_t, index=y_val.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:59:11.039599Z",
     "iopub.status.busy": "2025-11-18T00:59:11.038477Z",
     "iopub.status.idle": "2025-11-18T00:59:11.046065Z",
     "shell.execute_reply": "2025-11-18T00:59:11.045348Z",
     "shell.execute_reply.started": "2025-11-18T00:59:11.039568Z"
    },
    "id": "TaFE3dtqj6fT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OceanChemistryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        # y は pandas.Series でも numpy.ndarray でも受け付ける\n",
    "        y_arr = y.values if hasattr(y, 'values') else y\n",
    "        self.y = torch.tensor(y_arr, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = OceanChemistryDataset(X_train, y_train_proc)\n",
    "val_dataset = OceanChemistryDataset(X_val, y_val_proc)\n",
    "\n",
    "# === 調整ポイント: 学習のバッチ設定 ===\n",
    "# ・batch_size: 32/64/128/256 あたりで比較（大きいほど安定・速いが汎化は要CVで検証）→実際１が最強\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:59:13.716337Z",
     "iopub.status.busy": "2025-11-18T00:59:13.715598Z",
     "iopub.status.idle": "2025-11-18T00:59:13.724915Z",
     "shell.execute_reply": "2025-11-18T00:59:13.724300Z",
     "shell.execute_reply.started": "2025-11-18T00:59:13.716308Z"
    },
    "id": "8kUw3FU7j6fT",
    "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1163, 16)\n"
     ]
    }
   ],
   "source": [
    "# ===== MLPハイパラ（ここを主に調整） =====\n",
    "# DROPOUT_P: 0.1〜0.3 推奨（0で無効）\n",
    "DROPOUT_P = 0.1  # Dropout probability (0.1-0.3 recommended)\n",
    "# ACTIVATION_NAME: 'ReLU'|'GELU'|'SiLU'|'Tanh'|'LeakyReLU'|'SELU'|'ReLU6'\n",
    "ACTIVATION_NAME = 'SiLU'  # Options: ReLU, GELU, SiLU, Tanh, LeakyReLU, Mish, SELU, ReLU6\n",
    "\n",
    "def make_activation(name):\n",
    "    try:\n",
    "        key = (name or 'ReLU').lower()\n",
    "    except Exception:\n",
    "        key = 'relu'\n",
    "    if key == 'relu':\n",
    "        return nn.ReLU()\n",
    "    if key in ('mish','mesh'):\n",
    "        # Mish activation (fallback to SiLU if not available)\",\n",
    "        try:\n",
    "            return nn.Mish()\n",
    "        except AttributeError:\n",
    "            return nn.SiLU()\n",
    "    if key == 'gelu':\n",
    "        return nn.GELU()\n",
    "    if key in ('silu','swish'):\n",
    "        return nn.SiLU()\n",
    "    if key == 'selu':\n",
    "        return nn.SELU()\n",
    "    if key == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    if key in ('relu6','relu_6'):\n",
    "        return nn.ReLU6()\n",
    "    if key in ('leakyrelu','lrelu'):\n",
    "        return nn.LeakyReLU(0.01)\n",
    "    return nn.ReLU()\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME):\n",
    "\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        # 隠れ層ユニット数（今は1024。64〜1024で比較してみてください）\n",
    "        self.fc1 = nn.Linear(input_size, 2000)\n",
    "\n",
    "        self.act1 = make_activation(activation_name)\n",
    "\n",
    "        self.drop1 = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.fc3 = nn.Linear(2000, 1)  # Output layer for regression\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = MLPModel(input_size=X_train.shape[1], dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME)\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:59:17.344342Z",
     "iopub.status.busy": "2025-11-18T00:59:17.343597Z",
     "iopub.status.idle": "2025-11-18T01:25:07.413944Z",
     "shell.execute_reply": "2025-11-18T01:25:07.413124Z",
     "shell.execute_reply.started": "2025-11-18T00:59:17.344315Z"
    },
    "id": "TZrodbLrj6fT",
    "outputId": "15a041e0-6c82-455e-c55a-55bd117814ed",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:1042: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, LR: 1.00e-03, Train Loss: 1148.910692267863, Validation Loss: 492.32664454969034\n",
      "Epoch 101/5000, LR: 1.00e-03, Train Loss: 9.49483212542913, Validation Loss: 7.634780212878361\n",
      "Epoch 201/5000, LR: 5.00e-04, Train Loss: 4.837439912210378, Validation Loss: 6.180992867144727\n",
      "Epoch 301/5000, LR: 5.00e-04, Train Loss: 4.90464731785673, Validation Loss: 3.230524453740964\n",
      "Epoch 401/5000, LR: 2.50e-04, Train Loss: 2.9463938396049283, Validation Loss: 2.3252741839621485\n",
      "Epoch 501/5000, LR: 1.25e-04, Train Loss: 2.22259160338141, Validation Loss: 2.415731636058424\n",
      "Epoch 601/5000, LR: 6.25e-05, Train Loss: 1.8959213887707282, Validation Loss: 2.4484955279022147\n",
      "Epoch 701/5000, LR: 3.13e-05, Train Loss: 1.740083875174627, Validation Loss: 2.015378127008984\n",
      "Epoch 801/5000, LR: 7.81e-06, Train Loss: 1.6550669173869406, Validation Loss: 1.9976856326268302\n",
      "Epoch 901/5000, LR: 1.95e-06, Train Loss: 1.6365655703295539, Validation Loss: 1.9615266062191261\n",
      "Epoch 1001/5000, LR: 1.00e-06, Train Loss: 1.6289321820949134, Validation Loss: 1.958316629822926\n",
      "Early stopping at epoch 1100; best val: 1.947961\n"
     ]
    }
   ],
   "source": [
    "WEIGHT_DECAY = 1e-4  # 1e-4 to 1e-3 recommended\n",
    "EARLY_STOPPING_PATIENCE = 500  # epochs with no improvement before stop\n",
    "EARLY_STOPPING_MIN_DELTA = 1e-4  # minimum improvement to reset patience\n",
    "\n",
    "# Model selection toggle: 'BP' (PyTorch backprop), 'ELM', 'RBF'\n",
    "MODEL_NAME = 'BP'\n",
    "\n",
    "# ELM hyperparameters\n",
    "ELM_HIDDEN = 512\n",
    "ELM_REG = 1e-2  # ridge regularization\n",
    "ELM_ACTIVATION = 'relu'  # relu|tanh|sigmoid\n",
    "\n",
    "# RBF hyperparameters\n",
    "RBF_UNITS = 100\n",
    "RBF_REG = 1e-2\n",
    "RBF_SIGMA_SCALE = 1.0  # scale factor for sigma derived from centers\n",
    "\n",
    "import torch.optim as optim\n",
    "# ===== 最適化/損失/早期終了（主な調整ポイント） =====\n",
    "# OPTIMIZER_NAME: 'Adam'|'AdamW'|'SGD'|'RMSprop'|'Adagrad'|'NAdam'|'RAdam'\n",
    "#   ・SGDを使うなら OPTIMIZER_PARAMS={'momentum':0.9,'nesterov':True} など\n",
    "# WEIGHT_DECAY: 1e-4〜1e-3 推奨（L2正則化。大きすぎると学習が弱まる）\n",
    "# LOSS_NAME: 'SmoothL1'|'MSE'|'L1'|'Huber'（Huberはdelta、SmoothL1はbetaをLOSS_PARAMSで指定可）\n",
    "# EARLY_STOPPING_PATIENCE/MIN_DELTA: 早期終了の判定\n",
    "# 学習率lrはmake_optimizerの引数で指定。ReduceLROnPlateauで自動減衰\n",
    "\n",
    "# Optimizer toggle\n",
    "OPTIMIZER_NAME = 'AdamW'  # Options: 'Adam','AdamW','SGD','RMSprop','Adagrad','NAdam','RAdam'\n",
    "OPTIMIZER_PARAMS = {}  # e.g., {'momentum':0.9} for SGD\n",
    "\n",
    "def make_optimizer(name, params, **kwargs):\n",
    "    try:\n",
    "        key = (name or 'Adam').lower()\n",
    "    except Exception:\n",
    "        key = 'adam'\n",
    "    lr = kwargs.get('lr', 1e-3)\n",
    "    wd = kwargs.get('weight_decay', 0.0)\n",
    "    if key == 'adamw':\n",
    "        return optim.AdamW(params, lr=lr, weight_decay=wd)\n",
    "    if key == 'nadam':\n",
    "        try:\n",
    "            return optim.NAdam(params, lr=lr, weight_decay=wd)\n",
    "        except AttributeError:\n",
    "            # Fallback if NAdam is not available\n",
    "            return optim.Adam(params, lr=lr, weight_decay=wd)\n",
    "    if key == 'radam':\n",
    "        try:\n",
    "            return optim.RAdam(params, lr=lr, weight_decay=wd)\n",
    "        except AttributeError:\n",
    "            # Fallback if RAdam is not available\n",
    "            return optim.Adam(params, lr=lr, weight_decay=wd)\n",
    "    if key == 'sgd':\n",
    "        return optim.SGD(params, lr=lr, momentum=kwargs.get('momentum', 0.9), nesterov=kwargs.get('nesterov', False), weight_decay=wd)\n",
    "    if key == 'rmsprop':\n",
    "        return optim.RMSprop(params, lr=lr, momentum=kwargs.get('momentum', 0.0), alpha=kwargs.get('alpha', 0.99), weight_decay=wd)\n",
    "    if key == 'adagrad':\n",
    "        return optim.Adagrad(params, lr=lr, weight_decay=wd)\n",
    "    return optim.Adam(params, lr=lr, weight_decay=wd)\n",
    "\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "\n",
    "LOSS_NAME = 'SmoothL1'  # Options: 'SmoothL1', 'MSE', 'L1', 'Huber'\n",
    "LOSS_PARAMS = {}  # e.g., {'beta': 1.0} for SmoothL1 or {'delta': 1.0} for Huber\n",
    "\n",
    "def make_loss(name, **kwargs):\n",
    "    try:\n",
    "        key = (name or 'SmoothL1').lower()\n",
    "    except Exception:\n",
    "        key = 'smoothl1'\n",
    "    if key in ('mse','mseloss'):\n",
    "        return nn.MSELoss()\n",
    "    if key in ('l1','mae','l1loss'):\n",
    "        return nn.L1Loss()\n",
    "    if key in ('huber','huberloss'):\n",
    "        delta = kwargs.get('delta', 1.0)\n",
    "        try:\n",
    "            return nn.HuberLoss(delta=delta)\n",
    "        except TypeError:\n",
    "            return nn.SmoothL1Loss()\n",
    "    # Default SmoothL1\n",
    "    beta = kwargs.get('beta', 1.0)\n",
    "    try:\n",
    "        return nn.SmoothL1Loss(beta=beta)\n",
    "    except TypeError:\n",
    "        return nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "criterion = make_loss(LOSS_NAME, **LOSS_PARAMS)\n",
    "\n",
    "optimizer = make_optimizer(OPTIMIZER_NAME, model.parameters(), lr=0.001, weight_decay=WEIGHT_DECAY, **OPTIMIZER_PARAMS)\n",
    "\n",
    "# LR scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6)\n",
    "\n",
    "\n",
    "def _np_activation(name, X):\n",
    "    key = (name or 'relu').lower()\n",
    "    if key == 'relu':\n",
    "        return (X > 0) * X\n",
    "    if key == 'tanh':\n",
    "        return np.tanh(X)\n",
    "    if key in ('sigmoid','logistic'):\n",
    "        return 1.0 / (1.0 + np.exp(-X))\n",
    "    if key == 'selu':\n",
    "        alpha = 1.6732632423543772\n",
    "        scale = 1.0507009873554805\n",
    "        return scale * np.where(X > 0, X, alpha * (np.exp(X) - 1.0))\n",
    "    if key in ('relu6','relu_6'):\n",
    "        return np.minimum(np.maximum(X, 0), 6.0)\n",
    "    return (X > 0) * X\n",
    "\n",
    "def _ridge_solve(H, y, reg):\n",
    "    # Solve (H^T H + reg I) w = H^T y\n",
    "    HtH = H.T @ H\n",
    "    n = HtH.shape[0]\n",
    "    A = HtH + reg * np.eye(n)\n",
    "    b = H.T @ y\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "def fit_elm(X, y, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=42):\n",
    "    rs = np.random.RandomState(seed)\n",
    "    W = rs.normal(scale=1.0, size=(X.shape[1], hidden))\n",
    "    b = rs.normal(scale=1.0, size=(hidden,))\n",
    "    H = _np_activation(act, X @ W + b)\n",
    "    beta = _ridge_solve(H, y.astype(float), reg)\n",
    "    return {'W': W, 'b': b, 'beta': beta, 'act': act}\n",
    "\n",
    "def predict_elm(model_dict, X):\n",
    "    W = model_dict['W']; b = model_dict['b']; act = model_dict['act']; beta = model_dict['beta']\n",
    "    H = _np_activation(act, X @ W + b)\n",
    "    return H @ beta\n",
    "\n",
    "def _rbf_design(X, centers, gamma):\n",
    "    # Compute squared Euclidean distances efficiently\n",
    "    X2 = np.sum(X*X, axis=1, keepdims=True)\n",
    "    C2 = np.sum(centers*centers, axis=1)[None, :]\n",
    "    dist2 = X2 + C2 - 2.0 * (X @ centers.T)\n",
    "    return np.exp(-gamma * dist2)\n",
    "\n",
    "def fit_rbf(X, y, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=42):\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=units, random_state=seed, n_init=10)\n",
    "    centers = km.fit(X).cluster_centers_\n",
    "    # Estimate sigma from center distances\n",
    "    from scipy.spatial.distance import cdist\n",
    "    try:\n",
    "        import numpy as _np\n",
    "        pair = _np.linalg.norm(centers[:,None,:]-centers[None,:,:], axis=2)\n",
    "    except Exception:\n",
    "        pair = np.zeros((units, units))\n",
    "    # Use median of nearest-neighbor distances\n",
    "    nn = []\n",
    "    for i in range(units):\n",
    "        vals = np.sort(pair[i][pair[i]>0])\n",
    "        if vals.size>0:\n",
    "            nn.append(vals[0])\n",
    "    sigma = (np.median(nn) if len(nn)>0 else 1.0) * sigma_scale\n",
    "    sigma = max(sigma, 1e-6)\n",
    "    gamma = 1.0/(2.0*sigma*sigma)\n",
    "    Phi = _rbf_design(X, centers, gamma)\n",
    "    w = _ridge_solve(Phi, y.astype(float), reg)\n",
    "    return {'centers': centers, 'gamma': gamma, 'w': w}\n",
    "\n",
    "def predict_rbf(model_dict, X):\n",
    "    centers = model_dict['centers']; gamma = model_dict['gamma']; w = model_dict['w']\n",
    "    Phi = _rbf_design(X, centers, gamma)\n",
    "    return Phi @ w\n",
    "\n",
    "\n",
    "# Training function\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5000, scheduler=None):\n",
    "\n",
    "    import copy\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for X_batch, y_batch in val_loader:\n",
    "\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), y_batch)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss_avg = val_loss/len(val_loader) if len(val_loader)>0 else val_loss\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss_avg)\n",
    "\n",
    "        # Early stopping (only if we have validation batches)\n",
    "        if len(val_loader) > 0:\n",
    "            if best_val - val_loss_avg > EARLY_STOPPING_MIN_DELTA:\n",
    "                best_val = val_loss_avg\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch+1}; best val: {best_val:.6f}\")\n",
    "                if best_state is not None:\n",
    "                    model.load_state_dict(best_state)\n",
    "                break\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, LR: {current_lr:.2e}, Train Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss_avg}\")\n",
    "\n",
    "    # Load best state at the end if available\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "# Train based on selected model\n",
    "\n",
    "if MODEL_NAME.upper() == 'BP':\n",
    "    train_model(model, train_loader, val_loader, epochs=5000, scheduler=scheduler)\n",
    "elif MODEL_NAME.upper() == 'ELM':\n",
    "    print('Training ELM...')\n",
    "    ELM_MODEL = fit_elm(X_train, y_train, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=SEED)\n",
    "elif MODEL_NAME.upper() == 'RBF':\n",
    "    print('Training RBF...')\n",
    "    RBF_MODEL = fit_rbf(X_train, y_train, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=SEED)\n",
    "else:\n",
    "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T00:52:28.763783Z",
     "iopub.status.busy": "2025-11-18T00:52:28.763184Z",
     "iopub.status.idle": "2025-11-18T00:52:39.205194Z",
     "shell.execute_reply": "2025-11-18T00:52:39.204081Z",
     "shell.execute_reply.started": "2025-11-18T00:52:28.763757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:1042: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, LR: 1.00e-03, Train Loss: 1256.7205644178716, Validation Loss: 617.3388712145619\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/3029359920.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Train for this fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     train_model(fold_model, train_loader_cv, val_loader_cv,\n\u001b[0m\u001b[1;32m     65\u001b[0m                 epochs=5000, scheduler=scheduler)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/155631966.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, scheduler)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    244\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    876\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdevice_beta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===== KFold / OOF with per-fold preprocessing and test ensembling =====\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "N_SPLITS = 3\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_pred = np.zeros(len(X), dtype=np.float32)\n",
    "test_pred = np.zeros((N_SPLITS, len(test)), dtype=np.float32)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Fold {fold}/{N_SPLITS}\")\n",
    "\n",
    "    # Split raw features and target\n",
    "    X_tr_raw = X.iloc[tr_idx].copy()\n",
    "    X_val_raw = X.iloc[val_idx].copy()\n",
    "    y_tr = y.iloc[tr_idx]\n",
    "    y_val = y.iloc[val_idx]\n",
    "\n",
    "    # Imputer (fit on fold-train only)\n",
    "    imputer = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
    "    X_tr_imp = imputer.fit_transform(X_tr_raw)\n",
    "    X_val_imp = imputer.transform(X_val_raw)\n",
    "    test_imp = imputer.transform(test)\n",
    "\n",
    "    # Feature transform (fit on fold-train only)\n",
    "    ft = make_feature_transformer(FEATURE_TRANSFORM)\n",
    "    if ft is None:\n",
    "        X_tr_proc = X_tr_imp\n",
    "        X_val_proc = X_val_imp\n",
    "        test_proc = test_imp\n",
    "    else:\n",
    "        X_tr_proc = ft.fit_transform(X_tr_imp)\n",
    "        X_val_proc = ft.transform(X_val_imp)\n",
    "        test_proc = ft.transform(test_imp)\n",
    "\n",
    "    # Target transform (fit on fold-train only)\n",
    "    y_tr_t, target_params = fit_target_transform(y_tr, TARGET_TRANSFORM)\n",
    "    y_val_t = apply_target_transform(y_val, target_params)\n",
    "    y_tr_proc = pd.Series(y_tr_t, index=y_tr.index)\n",
    "    y_val_proc = pd.Series(y_val_t, index=y_val.index)\n",
    "\n",
    "    # Datasets and loaders\n",
    "    train_ds = OceanChemistryDataset(X_tr_proc, y_tr_proc)\n",
    "    val_ds = OceanChemistryDataset(X_val_proc, y_val_proc)\n",
    "    train_loader_cv = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "    val_loader_cv = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    # New model / optimizer / loss per fold\n",
    "    fold_model = MLPModel(input_size=X_tr_proc.shape[1],\n",
    "                          dropout_p=DROPOUT_P,\n",
    "                          activation_name=ACTIVATION_NAME)\n",
    "\n",
    "    # Use global names so train_model can see them\n",
    "    criterion = make_loss(LOSS_NAME, **LOSS_PARAMS)\n",
    "    optimizer = make_optimizer(OPTIMIZER_NAME, fold_model.parameters(),\n",
    "                               lr=0.001, weight_decay=WEIGHT_DECAY, **OPTIMIZER_PARAMS)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # Train for this fold\n",
    "    train_model(fold_model, train_loader_cv, val_loader_cv,\n",
    "                epochs=5000, scheduler=scheduler)\n",
    "\n",
    "    # OOF prediction (back to original target scale)\n",
    "    fold_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_tensor = torch.tensor(X_val_proc, dtype=torch.float32)\n",
    "        pred_val_t = fold_model(val_tensor).squeeze().cpu().numpy()\n",
    "    pred_val = inverse_target_transform(pred_val_t, target_params)\n",
    "    oof_pred[val_idx] = pred_val\n",
    "\n",
    "    # Test prediction for this fold (back to original scale)\n",
    "    with torch.no_grad():\n",
    "        test_tensor = torch.tensor(test_proc, dtype=torch.float32)\n",
    "        pred_test_t = fold_model(test_tensor).squeeze().cpu().numpy()\n",
    "    pred_test = inverse_target_transform(pred_test_t, target_params)\n",
    "    test_pred[fold - 1, :] = pred_test\n",
    "\n",
    "# OOF score on original scale\n",
    "mse = mean_squared_error(y.values, oof_pred)\n",
    "oof_rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"KFold OOF RMSE (full train): {oof_rmse:.4f}\")\n",
    "\n",
    "# Mean-ensemble test prediction over folds\n",
    "test_pred_mean = test_pred.mean(axis=0)\n",
    "\n",
    "# Submission using sample_submission id\n",
    "sub_kfold = sub.copy()\n",
    "sub_kfold[\"DIC\"] = test_pred_mean\n",
    "sub_kfold.to_csv(\"submission_kfold.csv\", index=False)\n",
    "print(\"Saved submission_kfold.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:30:20.046648Z",
     "iopub.status.busy": "2025-11-18T01:30:20.045964Z",
     "iopub.status.idle": "2025-11-18T01:30:20.053052Z",
     "shell.execute_reply": "2025-11-18T01:30:20.052429Z",
     "shell.execute_reply.started": "2025-11-18T01:30:20.046624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\": range(1455, 1455 + len(predictions)), \"DIC\": predictions})\n",
    "submission.to_csv(\"submission45.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:26:49.849053Z",
     "iopub.status.busy": "2025-11-18T01:26:49.848317Z",
     "iopub.status.idle": "2025-11-18T01:26:49.873714Z",
     "shell.execute_reply": "2025-11-18T01:26:49.873009Z",
     "shell.execute_reply.started": "2025-11-18T01:26:49.849029Z"
    },
    "id": "CD3YH6Drj6fT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict according to MODEL_NAME\n",
    "if MODEL_NAME.upper() == 'BP':\n",
    "    # Convert the test set into a torch tensor\n",
    "    test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_tensor).squeeze().numpy()\n",
    "elif MODEL_NAME.upper() == 'ELM':\n",
    "    predictions = predict_elm(ELM_MODEL, test_scaled)\n",
    "elif MODEL_NAME.upper() == 'RBF':\n",
    "    predictions = predict_rbf(RBF_MODEL, test_scaled)\n",
    "else:\n",
    "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n",
    "\n",
    "# Inverse target transform if applied (single split)\n",
    "try:\n",
    "    predictions = inverse_target_transform(predictions, TARGET_PARAMS_SINGLE)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\"id\": range(1455, 1455 + len(predictions)), \"DIC\": predictions})\n",
    "submission.to_csv(\"submission44.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7941427,
     "sourceId": 49552,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
