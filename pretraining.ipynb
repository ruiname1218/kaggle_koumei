{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seed_cell"
      },
      "outputs": [],
      "source": [
        "SEED = 5000\n",
        "import os, random, numpy as np\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "# PyTorch seeding (TensorFlow is skipped to avoid environment issues)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(SEED)\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:34.996422Z",
          "iopub.status.busy": "2024-10-31T16:06:34.995779Z",
          "iopub.status.idle": "2024-10-31T16:06:35.006468Z",
          "shell.execute_reply": "2024-10-31T16:06:35.005154Z",
          "shell.execute_reply.started": "2024-10-31T16:06:34.996355Z"
        },
        "id": "w0miHB_4j6fO",
        "outputId": "2b8535a0-4018-4af3-c73c-e0f06916eba3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\n",
            "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\n",
            "/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.009259Z",
          "iopub.status.busy": "2024-10-31T16:06:35.008896Z",
          "iopub.status.idle": "2024-10-31T16:06:35.040481Z",
          "shell.execute_reply": "2024-10-31T16:06:35.039428Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.009221Z"
        },
        "id": "wmcUA-Kyj6fR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
        "test=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
        "sub=pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install tabpfn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:35.160063Z",
          "iopub.status.busy": "2024-10-31T16:06:35.159559Z",
          "iopub.status.idle": "2024-10-31T16:06:38.637873Z",
          "shell.execute_reply": "2024-10-31T16:06:38.636808Z",
          "shell.execute_reply.started": "2024-10-31T16:06:35.160004Z"
        },
        "id": "2SYojsJUj6fS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler, QuantileTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Load data\n",
        "# train = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/train.csv\")\n",
        "# test = pd.read_csv(\"/kaggle/input/eds-232-ocean-chemistry-prediction-for-calcofi/test.csv\")\n",
        "\n",
        "# Drop unneeded columns and handle missing values\n",
        "train = train.drop(columns=[\"Unnamed: 12\", \"id\"])  # Dropping unnecessary columns\n",
        "\n",
        "# Find common columns between train and test (excluding \"DIC\" from train)\n",
        "common_columns = train.drop(columns=[\"DIC\"]).columns.intersection(test.columns)\n",
        "\n",
        "# Select the common columns for both train and test\n",
        "X = train[common_columns].copy()\n",
        "y = train[\"DIC\"]\n",
        "test = test[common_columns].copy()\n",
        "\n",
        "\n",
        "# Toggle for feature engineering (set to False to disable)\n",
        "# === 調整ポイント: 特徴量ENGのON/OFF（効果比較に便利） ===\n",
        "# True: 追加特徴（N_tot, 比率, 交互作用, sin/cos 等）を使う\n",
        "# False: 生の共通列のみで学習\n",
        "FE_ENABLED = False\n",
        "\n",
        "if FE_ENABLED:\n",
        "    # Feature engineering (deterministic; same for train/test)\n",
        "    # Totals and ratios (avoid divide-by-zero -> NaN; imputer will handle)\n",
        "    X['N_tot'] = X['NO3uM'] + X['NO2uM'] + X['NH3uM']\n",
        "    test['N_tot'] = test['NO3uM'] + test['NO2uM'] + test['NH3uM']\n",
        "    X['N_to_P'] = np.where(X['PO4uM'] == 0, np.nan, X['NO3uM'] / X['PO4uM'])\n",
        "    test['N_to_P'] = np.where(test['PO4uM'] == 0, np.nan, test['NO3uM'] / test['PO4uM'])\n",
        "    X['Si_to_N'] = np.where(X['NO3uM'] == 0, np.nan, X['SiO3uM'] / X['NO3uM'])\n",
        "    test['Si_to_N'] = np.where(test['NO3uM'] == 0, np.nan, test['SiO3uM'] / test['NO3uM'])\n",
        "    # Interactions\n",
        "    X['Depth_Temp'] = X['R_Depth'] * X['R_TEMP']\n",
        "    test['Depth_Temp'] = test['R_Depth'] * test['R_TEMP']\n",
        "    X['Sal_Temp'] = X['R_Sal'] * X['R_TEMP']\n",
        "    test['Sal_Temp'] = test['R_Sal'] * test['R_TEMP']\n",
        "    # Geographic trig features\n",
        "    X['sin_lat'] = np.sin(np.radians(X['Lat_Dec']))\n",
        "    X['cos_lat'] = np.cos(np.radians(X['Lat_Dec']))\n",
        "    X['sin_lon'] = np.sin(np.radians(X['Lon_Dec']))\n",
        "    X['cos_lon'] = np.cos(np.radians(X['Lon_Dec']))\n",
        "    test['sin_lat'] = np.sin(np.radians(test['Lat_Dec']))\n",
        "    test['cos_lat'] = np.cos(np.radians(test['Lat_Dec']))\n",
        "    test['sin_lon'] = np.sin(np.radians(test['Lon_Dec']))\n",
        "    test['cos_lon'] = np.cos(np.radians(test['Lon_Dec']))\n",
        "    # Replace infs with NaN to be imputed\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "    test = test.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# ===== 調整ポイント: 前処理/変換スイッチ =====\n",
        "# FEATURE_IMPUTER: 欠損補完の方法 ('mean' または 'median')\n",
        "# FEATURE_TRANSFORM: 特徴量変換\n",
        "#   - 'power'    : Yeo-Johnson + 標準化（0/負値OK, 初手におすすめ）\n",
        "#   - 'standard' : 標準化のみ（平均0・分散1）\n",
        "#   - 'robust'   : 外れ値に頑健なスケーリング\n",
        "#   - 'quantile' : 分位変換（出力分布は下のQUANTILE_OUTPUTで指定）\n",
        "#   - 'none'     : 変換しない\n",
        "# TARGET_TRANSFORM: 目的変数変換 ('none'|'log1p'|'standard')\n",
        "#   - 'log1p'    : スケール/外れ値を抑制。予測は自動でexpm1逆変換\n",
        "#   - 'standard' : 平均0・分散1に正規化（予測は平均・分散で戻す）\n",
        "# 変更後は前処理→学習→推論の順に実行してください\n",
        "# Switches for feature/target transforms\n",
        "FEATURE_IMPUTER = 'mean'  # 'mean' or 'median'\n",
        "FEATURE_TRANSFORM = 'power'  # 'power'|'standard'|'robust'|'quantile'|'none'\n",
        "QUANTILE_OUTPUT = 'normal'  # 'normal' or 'uniform'\n",
        "\n",
        "TARGET_TRANSFORM = 'none'  # 'none'|'log1p'|'standard'\n",
        "\n",
        "# ===== TabPFN stacking feature (optional) =====\n",
        "# 事前学習済みのTabPFNからOOF予測を作り、1次元の補助特徴量として追加\n",
        "STACK_TABPFN = True  # Falseで無効化\n",
        "TABPFN_N_SPLITS = 5\n",
        "TABPFN_ENSEMBLE = 16\n",
        "TABPFN_BINS = 30  # Regressorが無い環境では分類のビニングで近似\n",
        "\n",
        "def add_tabpfn_stack_feature(X_df, y_series, test_df, n_splits=TABPFN_N_SPLITS):\n",
        "    try:\n",
        "        from tabpfn import TabPFNRegressor, TabPFNClassifier\n",
        "    except Exception as e:\n",
        "        # 必要ならインストールを試行（失敗したらスキップ）\n",
        "        try:\n",
        "            import sys, subprocess\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'tabpfn'])\n",
        "            from tabpfn import TabPFNRegressor, TabPFNClassifier\n",
        "        except Exception as ie:\n",
        "            print('TabPFN not available; skipping stacking. Error:', ie)\n",
        "            return X_df, test_df, None\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "    oof = np.zeros(len(X_df), dtype=float)\n",
        "    test_pred_folds = []\n",
        "    X_np = X_df.values\n",
        "    test_np = test_df.values\n",
        "    y_np = y_series.values if hasattr(y_series, 'values') else np.asarray(y_series)\n",
        "    for tr_idx, va_idx in kf.split(X_np):\n",
        "        X_tr = X_np[tr_idx]\n",
        "        X_va = X_np[va_idx]\n",
        "        y_tr = y_np[tr_idx]\n",
        "        # 欠損は学習FoldでImputerをfit（リーク防止）\n",
        "        imp = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
        "        X_tr_imp = imp.fit_transform(X_tr)\n",
        "        X_va_imp = imp.transform(X_va)\n",
        "        test_imp_local = imp.transform(test_np)\n",
        "        try:\n",
        "            # 回帰器が利用可能な場合\n",
        "            reg = TabPFNRegressor(N_ensemble_configurations=TABPFN_ENSEMBLE, seed=SEED, device=device)\n",
        "            reg.fit(X_tr_imp, y_tr)\n",
        "            oof[va_idx] = reg.predict(X_va_imp).astype(float)\n",
        "            test_pred = reg.predict(test_imp_local).astype(float)\n",
        "        except Exception as e:\n",
        "            # 分類の確率出力で回帰を近似（分位ビンの期待値）\n",
        "            bins = np.quantile(y_tr, np.linspace(0.0, 1.0, TABPFN_BINS + 1))\n",
        "            bins[0] = -np.inf\n",
        "            bins[-1] = np.inf\n",
        "            y_tr_binned = np.digitize(y_tr, bins[1:-1], right=True)\n",
        "            clf = TabPFNClassifier(N_ensemble_configurations=TABPFN_ENSEMBLE, seed=SEED, device=device)\n",
        "            clf.fit(X_tr_imp, y_tr_binned)\n",
        "            proba_va = clf.predict_proba(X_va_imp)\n",
        "            proba_te = clf.predict_proba(test_imp_local)\n",
        "            mids_all = (bins[:-1] + bins[1:]) / 2.0\n",
        "            try:\n",
        "                classes = clf.classes_.astype(int)\n",
        "                mids_used = mids_all[classes]\n",
        "            except Exception:\n",
        "                mids_used = mids_all\n",
        "            oof[va_idx] = (proba_va * mids_used).sum(axis=1)\n",
        "            test_pred = (proba_te * mids_used).sum(axis=1)\n",
        "        test_pred_folds.append(test_pred)\n",
        "    test_pred_mean = np.mean(np.column_stack(test_pred_folds), axis=1)\n",
        "    X_new = X_df.copy()\n",
        "    test_new = test_df.copy()\n",
        "    X_new['tabpfn_pred'] = oof\n",
        "    test_new['tabpfn_pred'] = test_pred_mean\n",
        "    return X_new, test_new, oof\n",
        "\n",
        "# 学習/検証分割の前にTabPFN特徴を追加\n",
        "if 'STACK_TABPFN' not in globals():\n",
        "    STACK_TABPFN = True\n",
        "if STACK_TABPFN:\n",
        "    try:\n",
        "        X, test, _ = add_tabpfn_stack_feature(X, y, test)\n",
        "        print('TabPFN stacked feature added: column tabpfn_pred')\n",
        "    except Exception as e:\n",
        "        print('TabPFN stacking failed; continuing without it. Error:', e)\n",
        "\n",
        "def make_feature_transformer(name):\n",
        "    key = (name or 'none').lower()\n",
        "    if key == 'standard':\n",
        "        return StandardScaler()\n",
        "    if key == 'robust':\n",
        "        return RobustScaler()\n",
        "    if key == 'quantile':\n",
        "        return QuantileTransformer(output_distribution=QUANTILE_OUTPUT, random_state=SEED)\n",
        "    if key == 'power':\n",
        "        return PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "    return None\n",
        "\n",
        "def fit_target_transform(y, name):\n",
        "    key = (name or 'none').lower()\n",
        "    yy = pd.Series(y).astype(float)\n",
        "    if key == 'log1p':\n",
        "        return np.log1p(yy).astype(np.float32).values, {'name':'log1p'}\n",
        "    if key == 'standard':\n",
        "        mu = float(yy.mean()); sd = float(yy.std() + 1e-8)\n",
        "        return ((yy - mu)/sd).astype(np.float32).values, {'name':'standard','mean':mu,'std':sd}\n",
        "    return yy.astype(np.float32).values, {'name':'none'}\n",
        "\n",
        "def apply_target_transform(y, params):\n",
        "    key = (params.get('name') or 'none').lower()\n",
        "    yy = pd.Series(y).astype(float)\n",
        "    if key == 'log1p':\n",
        "        return np.log1p(yy).astype(np.float32).values\n",
        "    if key == 'standard':\n",
        "        mu = params['mean']; sd = params['std']\n",
        "        return ((yy - mu)/sd).astype(np.float32).values\n",
        "    return yy.astype(np.float32).values\n",
        "\n",
        "def inverse_target_transform(arr, params):\n",
        "    key = (params.get('name') or 'none').lower()\n",
        "    a = np.asarray(arr, dtype=float)\n",
        "    if key == 'log1p':\n",
        "        return np.expm1(a)\n",
        "    if key == 'standard':\n",
        "        return a * params['std'] + params['mean']\n",
        "    return a\n",
        "\n",
        "# Split first to avoid leakage\n",
        "X_train_raw, X_val_raw, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Impute missing values (fit on train only)\n",
        "imputer = SimpleImputer(strategy=FEATURE_IMPUTER)\n",
        "X_train_imp = imputer.fit_transform(X_train_raw)\n",
        "X_val_imp = imputer.transform(X_val_raw)\n",
        "test_imp = imputer.transform(test)\n",
        "\n",
        "# Feature transform (fit on train only)\n",
        "_ft = make_feature_transformer(FEATURE_TRANSFORM)\n",
        "if _ft is None:\n",
        "    X_train = X_train_imp\n",
        "    X_val = X_val_imp\n",
        "    test_scaled = test_imp\n",
        "else:\n",
        "    X_train = _ft.fit_transform(X_train_imp)\n",
        "    X_val = _ft.transform(X_val_imp)\n",
        "    test_scaled = _ft.transform(test_imp)\n",
        "\n",
        "# Target transform (fit on train only)\n",
        "y_train_t, TARGET_PARAMS_SINGLE = fit_target_transform(y_train, TARGET_TRANSFORM)\n",
        "y_val_t = apply_target_transform(y_val, TARGET_PARAMS_SINGLE)\n",
        "y_train_proc = pd.Series(y_train_t, index=y_train.index)\n",
        "y_val_proc = pd.Series(y_val_t, index=y_val.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:38.640046Z",
          "iopub.status.busy": "2024-10-31T16:06:38.639302Z",
          "iopub.status.idle": "2024-10-31T16:06:38.677768Z",
          "shell.execute_reply": "2024-10-31T16:06:38.676875Z",
          "shell.execute_reply.started": "2024-10-31T16:06:38.639993Z"
        },
        "id": "TaFE3dtqj6fT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class OceanChemistryDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        # y は pandas.Series でも numpy.ndarray でも受け付ける\n",
        "        y_arr = y.values if hasattr(y, 'values') else y\n",
        "        self.y = torch.tensor(y_arr, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = OceanChemistryDataset(X_train, y_train_proc)\n",
        "val_dataset = OceanChemistryDataset(X_val, y_val_proc)\n",
        "\n",
        "# === 調整ポイント: 学習のバッチ設定 ===\n",
        "# ・batch_size: 32/64/128/256 あたりで比較（大きいほど安定・速いが汎化は要CVで検証）→実際１が最強\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:38.679793Z",
          "iopub.status.busy": "2024-10-31T16:06:38.679208Z",
          "iopub.status.idle": "2024-10-31T16:06:38.702210Z",
          "shell.execute_reply": "2024-10-31T16:06:38.701153Z",
          "shell.execute_reply.started": "2024-10-31T16:06:38.679681Z"
        },
        "id": "8kUw3FU7j6fT",
        "outputId": "a9af3f6d-335d-4a85-c2c1-bf760562a6b5",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1163, 15)\n"
          ]
        }
      ],
      "source": [
        "# ===== MLPハイパラ（ここを主に調整） =====\n",
        "# DROPOUT_P: 0.1〜0.3 推奨（0で無効）\n",
        "DROPOUT_P = 0  # Dropout probability (0.1-0.3 recommended)\n",
        "# ACTIVATION_NAME: 'ReLU'|'GELU'|'SiLU'|'Tanh'|'LeakyReLU'\n",
        "ACTIVATION_NAME = 'Tanh'  # Options: ReLU, GELU, SiLU, Tanh, LeakyReLU\n",
        "\n",
        "def make_activation(name):\n",
        "    try:\n",
        "        key = (name or 'ReLU').lower()\n",
        "    except Exception:\n",
        "        key = 'relu'\n",
        "    if key == 'relu':\n",
        "        return nn.ReLU()\n",
        "    if key == 'gelu':\n",
        "        return nn.GELU()\n",
        "    if key in ('silu','swish'):\n",
        "        return nn.SiLU()\n",
        "    if key == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    if key in ('leakyrelu','lrelu'):\n",
        "        return nn.LeakyReLU(0.01)\n",
        "    return nn.ReLU()\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME):\n",
        "\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        # 隠れ層ユニット数（今は1024。64〜1024で比較してみてください）\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "\n",
        "        self.act1 = make_activation(activation_name)\n",
        "\n",
        "        self.drop1 = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.fc3 = nn.Linear(1024, 1)  # Output layer for regression\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "model = MLPModel(input_size=X_train.shape[1], dropout_p=DROPOUT_P, activation_name=ACTIVATION_NAME)\n",
        "\n",
        "print(X_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:06:38.705593Z",
          "iopub.status.busy": "2024-10-31T16:06:38.705176Z",
          "iopub.status.idle": "2024-10-31T16:09:35.511105Z",
          "shell.execute_reply": "2024-10-31T16:09:35.509987Z",
          "shell.execute_reply.started": "2024-10-31T16:06:38.705553Z"
        },
        "id": "TZrodbLrj6fT",
        "outputId": "15a041e0-6c82-455e-c55a-55bd117814ed",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5000, Train Loss: 4633887.947368421, Validation Loss: 4612593.2\n",
            "Epoch 101/5000, Train Loss: 15084.370579769737, Validation Loss: 18933.8005859375\n",
            "Epoch 201/5000, Train Loss: 2547.9099185341283, Validation Loss: 3594.0954345703126\n",
            "Epoch 301/5000, Train Loss: 716.5324530350534, Validation Loss: 948.5962158203125\n",
            "Epoch 401/5000, Train Loss: 400.37651142321135, Validation Loss: 575.134521484375\n",
            "Epoch 501/5000, Train Loss: 284.9509711014597, Validation Loss: 413.65491943359376\n",
            "Epoch 601/5000, Train Loss: 157.120555676912, Validation Loss: 215.35331115722656\n",
            "Epoch 701/5000, Train Loss: 115.85318153782895, Validation Loss: 136.43557891845703\n",
            "Epoch 801/5000, Train Loss: 78.71800462823165, Validation Loss: 102.47501983642579\n",
            "Epoch 901/5000, Train Loss: 79.39923437018143, Validation Loss: 84.50692291259766\n",
            "Epoch 1001/5000, Train Loss: 61.57915757831774, Validation Loss: 90.80420532226563\n",
            "Epoch 1101/5000, Train Loss: 59.011202460841126, Validation Loss: 79.94369735717774\n",
            "Epoch 1201/5000, Train Loss: 44.262922738727774, Validation Loss: 54.60122985839844\n",
            "Epoch 1301/5000, Train Loss: 45.66606300755551, Validation Loss: 71.30249328613282\n",
            "Epoch 1401/5000, Train Loss: 39.66262847498844, Validation Loss: 62.98748016357422\n",
            "Epoch 1501/5000, Train Loss: 32.760126063698216, Validation Loss: 51.929491424560545\n",
            "Epoch 1601/5000, Train Loss: 34.770258250989414, Validation Loss: 45.674285125732425\n",
            "Epoch 1701/5000, Train Loss: 34.51503743623432, Validation Loss: 46.07814178466797\n",
            "Epoch 1801/5000, Train Loss: 43.35380002071983, Validation Loss: 48.40091400146484\n",
            "Epoch 1901/5000, Train Loss: 34.933612622712786, Validation Loss: 122.43202896118164\n",
            "Epoch 2001/5000, Train Loss: 34.670268761484245, Validation Loss: 72.05145797729492\n",
            "Epoch 2101/5000, Train Loss: 35.82535342166298, Validation Loss: 44.939079666137694\n",
            "Epoch 2201/5000, Train Loss: 37.402690636484245, Validation Loss: 73.06920852661133\n",
            "Epoch 2301/5000, Train Loss: 43.179393667923776, Validation Loss: 64.67569885253906\n",
            "Epoch 2401/5000, Train Loss: 26.058987617492676, Validation Loss: 45.704331970214845\n",
            "Epoch 2501/5000, Train Loss: 27.63914273914538, Validation Loss: 44.29252738952637\n",
            "Epoch 2601/5000, Train Loss: 27.794708753886976, Validation Loss: 59.06520156860351\n",
            "Epoch 2701/5000, Train Loss: 31.337925659982783, Validation Loss: 71.96796264648438\n",
            "Epoch 2801/5000, Train Loss: 22.820767503035697, Validation Loss: 35.30337066650391\n",
            "Epoch 2901/5000, Train Loss: 27.68800253617136, Validation Loss: 49.846096420288085\n",
            "Epoch 3001/5000, Train Loss: 60.380249675951504, Validation Loss: 59.2910717010498\n",
            "Epoch 3101/5000, Train Loss: 36.74450937070345, Validation Loss: 43.3547061920166\n",
            "Epoch 3201/5000, Train Loss: 30.514942068802682, Validation Loss: 38.17370433807373\n",
            "Epoch 3301/5000, Train Loss: 23.526791999214574, Validation Loss: 39.81262474060058\n",
            "Epoch 3401/5000, Train Loss: 26.541675567626953, Validation Loss: 46.78069152832031\n",
            "Epoch 3501/5000, Train Loss: 30.57578518516139, Validation Loss: 43.97907066345215\n",
            "Epoch 3601/5000, Train Loss: 25.92467097232216, Validation Loss: 47.50724105834961\n",
            "Epoch 3701/5000, Train Loss: 26.10161946949206, Validation Loss: 45.497413063049315\n",
            "Epoch 3801/5000, Train Loss: 21.197749087685033, Validation Loss: 44.4531192779541\n",
            "Epoch 3901/5000, Train Loss: 23.96062886087518, Validation Loss: 38.40642166137695\n",
            "Epoch 4001/5000, Train Loss: 21.920030719355534, Validation Loss: 43.43920211791992\n",
            "Epoch 4101/5000, Train Loss: 25.60328365627088, Validation Loss: 43.016561889648436\n",
            "Epoch 4201/5000, Train Loss: 21.379806066814222, Validation Loss: 46.96633796691894\n",
            "Epoch 4301/5000, Train Loss: 20.608908954419586, Validation Loss: 47.01842346191406\n",
            "Epoch 4401/5000, Train Loss: 28.0404204067431, Validation Loss: 42.011796951293945\n",
            "Epoch 4501/5000, Train Loss: 23.913236266688298, Validation Loss: 45.37639617919922\n",
            "Epoch 4601/5000, Train Loss: 29.726560065620824, Validation Loss: 117.87635116577148\n",
            "Epoch 4701/5000, Train Loss: 19.73130635211342, Validation Loss: 40.123684883117676\n",
            "Epoch 4801/5000, Train Loss: 18.472513612947967, Validation Loss: 41.08392143249512\n",
            "Epoch 4901/5000, Train Loss: 19.049142159913714, Validation Loss: 40.933606719970705\n"
          ]
        }
      ],
      "source": [
        "WEIGHT_DECAY = 1e-4  # 1e-4 to 1e-3 recommended\n",
        "EARLY_STOPPING_PATIENCE = 200  # epochs with no improvement before stop\n",
        "EARLY_STOPPING_MIN_DELTA = 1e-4  # minimum improvement to reset patience\n",
        "\n",
        "# Model selection toggle: 'BP' (PyTorch backprop), 'ELM', 'RBF'\n",
        "MODEL_NAME = 'BP'\n",
        "\n",
        "# ELM hyperparameters\n",
        "ELM_HIDDEN = 512\n",
        "ELM_REG = 1e-2  # ridge regularization\n",
        "ELM_ACTIVATION = 'relu'  # relu|tanh|sigmoid\n",
        "\n",
        "# RBF hyperparameters\n",
        "RBF_UNITS = 100\n",
        "RBF_REG = 1e-2\n",
        "RBF_SIGMA_SCALE = 1.0  # scale factor for sigma derived from centers\n",
        "\n",
        "import torch.optim as optim\n",
        "# ===== 最適化/損失/早期終了（主な調整ポイント） =====\n",
        "# OPTIMIZER_NAME: 'Adam'|'AdamW'|'SGD'|'RMSprop'|'Adagrad'\n",
        "#   ・SGDを使うなら OPTIMIZER_PARAMS={'momentum':0.9,'nesterov':True} など\n",
        "# WEIGHT_DECAY: 1e-4〜1e-3 推奨（L2正則化。大きすぎると学習が弱まる）\n",
        "# LOSS_NAME: 'SmoothL1'|'MSE'|'L1'|'Huber'（Huberはdelta、SmoothL1はbetaをLOSS_PARAMSで指定可）\n",
        "# EARLY_STOPPING_PATIENCE/MIN_DELTA: 早期終了の判定\n",
        "# 学習率lrはmake_optimizerの引数で指定。ReduceLROnPlateauで自動減衰\n",
        "\n",
        "# Optimizer toggle\n",
        "OPTIMIZER_NAME = 'AdamW'  # Options: 'Adam','AdamW','SGD','RMSprop','Adagrad'\n",
        "OPTIMIZER_PARAMS = {}  # e.g., {'momentum':0.9} for SGD\n",
        "\n",
        "def make_optimizer(name, params, **kwargs):\n",
        "    try:\n",
        "        key = (name or 'Adam').lower()\n",
        "    except Exception:\n",
        "        key = 'adam'\n",
        "    lr = kwargs.get('lr', 1e-3)\n",
        "    wd = kwargs.get('weight_decay', 0.0)\n",
        "    if key == 'adamw':\n",
        "        return optim.AdamW(params, lr=lr, weight_decay=wd)\n",
        "    if key == 'sgd':\n",
        "        return optim.SGD(params, lr=lr, momentum=kwargs.get('momentum', 0.9), nesterov=kwargs.get('nesterov', False), weight_decay=wd)\n",
        "    if key == 'rmsprop':\n",
        "        return optim.RMSprop(params, lr=lr, momentum=kwargs.get('momentum', 0.0), alpha=kwargs.get('alpha', 0.99), weight_decay=wd)\n",
        "    if key == 'adagrad':\n",
        "        return optim.Adagrad(params, lr=lr, weight_decay=wd)\n",
        "    return optim.Adam(params, lr=lr, weight_decay=wd)\n",
        "\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "\n",
        "LOSS_NAME = 'SmoothL1'  # Options: 'SmoothL1', 'MSE', 'L1', 'Huber'\n",
        "LOSS_PARAMS = {}  # e.g., {'beta': 1.0} for SmoothL1 or {'delta': 1.0} for Huber\n",
        "\n",
        "def make_loss(name, **kwargs):\n",
        "    try:\n",
        "        key = (name or 'SmoothL1').lower()\n",
        "    except Exception:\n",
        "        key = 'smoothl1'\n",
        "    if key in ('mse','mseloss'):\n",
        "        return nn.MSELoss()\n",
        "    if key in ('l1','mae','l1loss'):\n",
        "        return nn.L1Loss()\n",
        "    if key in ('huber','huberloss'):\n",
        "        delta = kwargs.get('delta', 1.0)\n",
        "        try:\n",
        "            return nn.HuberLoss(delta=delta)\n",
        "        except TypeError:\n",
        "            return nn.SmoothL1Loss()\n",
        "    # Default SmoothL1\n",
        "    beta = kwargs.get('beta', 1.0)\n",
        "    try:\n",
        "        return nn.SmoothL1Loss(beta=beta)\n",
        "    except TypeError:\n",
        "        return nn.SmoothL1Loss()\n",
        "\n",
        "\n",
        "criterion = make_loss(LOSS_NAME, **LOSS_PARAMS)\n",
        "\n",
        "optimizer = make_optimizer(OPTIMIZER_NAME, model.parameters(), lr=0.001, weight_decay=WEIGHT_DECAY, **OPTIMIZER_PARAMS)\n",
        "\n",
        "# LR scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-6)\n",
        "\n",
        "\n",
        "def _np_activation(name, X):\n",
        "    key = (name or 'relu').lower()\n",
        "    if key == 'relu':\n",
        "        return (X > 0) * X\n",
        "    if key == 'tanh':\n",
        "        return np.tanh(X)\n",
        "    if key in ('sigmoid','logistic'):\n",
        "        return 1.0 / (1.0 + np.exp(-X))\n",
        "    return (X > 0) * X\n",
        "\n",
        "def _ridge_solve(H, y, reg):\n",
        "    # Solve (H^T H + reg I) w = H^T y\n",
        "    HtH = H.T @ H\n",
        "    n = HtH.shape[0]\n",
        "    A = HtH + reg * np.eye(n)\n",
        "    b = H.T @ y\n",
        "    return np.linalg.solve(A, b)\n",
        "\n",
        "def fit_elm(X, y, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=42):\n",
        "    rs = np.random.RandomState(seed)\n",
        "    W = rs.normal(scale=1.0, size=(X.shape[1], hidden))\n",
        "    b = rs.normal(scale=1.0, size=(hidden,))\n",
        "    H = _np_activation(act, X @ W + b)\n",
        "    beta = _ridge_solve(H, y.astype(float), reg)\n",
        "    return {'W': W, 'b': b, 'beta': beta, 'act': act}\n",
        "\n",
        "def predict_elm(model_dict, X):\n",
        "    W = model_dict['W']; b = model_dict['b']; act = model_dict['act']; beta = model_dict['beta']\n",
        "    H = _np_activation(act, X @ W + b)\n",
        "    return H @ beta\n",
        "\n",
        "def _rbf_design(X, centers, gamma):\n",
        "    # Compute squared Euclidean distances efficiently\n",
        "    X2 = np.sum(X*X, axis=1, keepdims=True)\n",
        "    C2 = np.sum(centers*centers, axis=1)[None, :]\n",
        "    dist2 = X2 + C2 - 2.0 * (X @ centers.T)\n",
        "    return np.exp(-gamma * dist2)\n",
        "\n",
        "def fit_rbf(X, y, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=42):\n",
        "    from sklearn.cluster import KMeans\n",
        "    km = KMeans(n_clusters=units, random_state=seed, n_init=10)\n",
        "    centers = km.fit(X).cluster_centers_\n",
        "    # Estimate sigma from center distances\n",
        "    from scipy.spatial.distance import cdist\n",
        "    try:\n",
        "        import numpy as _np\n",
        "        pair = _np.linalg.norm(centers[:,None,:]-centers[None,:,:], axis=2)\n",
        "    except Exception:\n",
        "        pair = np.zeros((units, units))\n",
        "    # Use median of nearest-neighbor distances\n",
        "    nn = []\n",
        "    for i in range(units):\n",
        "        vals = np.sort(pair[i][pair[i]>0])\n",
        "        if vals.size>0:\n",
        "            nn.append(vals[0])\n",
        "    sigma = (np.median(nn) if len(nn)>0 else 1.0) * sigma_scale\n",
        "    sigma = max(sigma, 1e-6)\n",
        "    gamma = 1.0/(2.0*sigma*sigma)\n",
        "    Phi = _rbf_design(X, centers, gamma)\n",
        "    w = _ridge_solve(Phi, y.astype(float), reg)\n",
        "    return {'centers': centers, 'gamma': gamma, 'w': w}\n",
        "\n",
        "def predict_rbf(model_dict, X):\n",
        "    centers = model_dict['centers']; gamma = model_dict['gamma']; w = model_dict['w']\n",
        "    Phi = _rbf_design(X, centers, gamma)\n",
        "    return Phi @ w\n",
        "\n",
        "\n",
        "# Training function\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5000, scheduler=None):\n",
        "\n",
        "    import copy\n",
        "    best_val = float('inf')\n",
        "    best_state = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "\n",
        "            loss = criterion(outputs.squeeze(), y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        val_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for X_batch, y_batch in val_loader:\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "\n",
        "                loss = criterion(outputs.squeeze(), y_batch)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss_avg = val_loss/len(val_loader) if len(val_loader)>0 else val_loss\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss_avg)\n",
        "\n",
        "        # Early stopping (only if we have validation batches)\n",
        "        if len(val_loader) > 0:\n",
        "            if best_val - val_loss_avg > EARLY_STOPPING_MIN_DELTA:\n",
        "                best_val = val_loss_avg\n",
        "                best_state = copy.deepcopy(model.state_dict())\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch+1}; best val: {best_val:.6f}\")\n",
        "                if best_state is not None:\n",
        "                    model.load_state_dict(best_state)\n",
        "                break\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, LR: {current_lr:.2e}, Train Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss_avg}\")\n",
        "\n",
        "    # Load best state at the end if available\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "# Train based on selected model\n",
        "\n",
        "if MODEL_NAME.upper() == 'BP':\n",
        "    train_model(model, train_loader, val_loader, epochs=5000, scheduler=scheduler)\n",
        "elif MODEL_NAME.upper() == 'ELM':\n",
        "    print('Training ELM...')\n",
        "    ELM_MODEL = fit_elm(X_train, y_train, hidden=ELM_HIDDEN, reg=ELM_REG, act=ELM_ACTIVATION, seed=SEED)\n",
        "elif MODEL_NAME.upper() == 'RBF':\n",
        "    print('Training RBF...')\n",
        "    RBF_MODEL = fit_rbf(X_train, y_train, units=RBF_UNITS, reg=RBF_REG, sigma_scale=RBF_SIGMA_SCALE, seed=SEED)\n",
        "else:\n",
        "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-31T16:09:35.513910Z",
          "iopub.status.busy": "2024-10-31T16:09:35.512949Z",
          "iopub.status.idle": "2024-10-31T16:09:35.534684Z",
          "shell.execute_reply": "2024-10-31T16:09:35.533508Z",
          "shell.execute_reply.started": "2024-10-31T16:09:35.513841Z"
        },
        "id": "CD3YH6Drj6fT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Predict according to MODEL_NAME\n",
        "if MODEL_NAME.upper() == 'BP':\n",
        "    # Convert the test set into a torch tensor\n",
        "    test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(test_tensor).squeeze().numpy()\n",
        "elif MODEL_NAME.upper() == 'ELM':\n",
        "    predictions = predict_elm(ELM_MODEL, test_scaled)\n",
        "elif MODEL_NAME.upper() == 'RBF':\n",
        "    predictions = predict_rbf(RBF_MODEL, test_scaled)\n",
        "else:\n",
        "    raise ValueError(f'Unknown MODEL_NAME: {MODEL_NAME}')\n",
        "\n",
        "# Inverse target transform if applied (single split)\n",
        "try:\n",
        "    predictions = inverse_target_transform(predictions, TARGET_PARAMS_SINGLE)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Prepare submission\n",
        "submission = pd.DataFrame({\"id\": range(1455, 1455 + len(predictions)), \"DIC\": predictions})\n",
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7941427,
          "sourceId": 49552,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
